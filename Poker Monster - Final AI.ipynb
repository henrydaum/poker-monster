{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ed11a4",
   "metadata": {},
   "source": [
    "# Poker Monster\n",
    "by Henry Daum\n",
    "\n",
    "This is the game engine and deep reinforcement learning AI for my card game Poker Monster.\n",
    "\n",
    "(The following code cell initializes some basic variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9612ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In order to implement more cards, here is what's needed:\n",
    "Some cards create rule changes that can be done in the is_legal method of the action class.\n",
    "Some cards specifically target the hero or monster, which can be done in the action class.\n",
    "Some cards have special effects that can be implemented as a Card subclass.\n",
    "Some cards need additional information to play (are multi-step), which can be handled using \n",
    "game phases and writing additional information to 'the cache' in each phase, as well as\n",
    "using action subclasses.\"\"\"\n",
    "\n",
    "# This is the data for the cards in the game.\n",
    "hero_card_data = [\n",
    "    (3, \"Awakening\", \"hero\", 3, \"short\", None, \"Flip over your power cards, revealing them. Any that are short cards return to your hand. Any that are long cards stay on the board face-up.\"),\n",
    "    (3, \"Healthy Eating\", \"hero\", 2, \"short\", None, \"Draw a card. You can play an extra power card this turn.\"),\n",
    "    (2, \"The Sun\", \"hero\", 2, \"long\", 2, \"The Monster can only play 1 face-up card per turn.\"),\n",
    "    (2, \"The Moon\", \"hero\", 3, \"long\", 2, \"The Monster can't play any more power cards.\"),\n",
    "    (2, \"A Playful Pixie\", \"hero\", 4, \"long\", 4, \"At the start of your turn, you get to steal the top card of the Monster's deck. (Repeat this every turn.)\"),\n",
    "    (2, \"A Pearlescent Dragon\", \"hero\", 5, \"long\", 4, \"At the start of your turn, you get to steal 5 health from the Monster. (Repeat this every turn.)\"),\n",
    "    (1, \"Last Stand\", \"hero\", 0, \"short\", None, \"Shuffle 3 other cards from your discard pile into your deck. Until your next turn starts, your health can't reach 0 (damage that would put it to less than 1 puts it to 1 instead.)\"),\n",
    "    (2, \"Reconsider\", \"hero\", 1, \"short\", None, \"Look at the top 3 cards of your deck, and put them back in any order you choose.\"),\n",
    "    (3, \"Noble Sacrifice\", \"hero\", 1, \"short\", None, \"As an additional cost to play this card, you must sacrifice a long card. Look at your opponent's hand and discard a card from it.\")\n",
    "]\n",
    "\"\"\"The data format is: quantity, name, owner, power_cost, card_type, health, card text\"\"\"\n",
    "\n",
    "monster_card_data = [\n",
    "    (3, \"Monster's Pawn\", \"monster\", 3, \"long\", 3, \"Your first short card each turn costs no power to play.\"),\n",
    "    (1, \"Power Trip\", \"monster\", 0, \"short\", None, \"Gain +2 power (for this turn only).\"),\n",
    "    (3, \"Go All In\", \"monster\", 3, \"short\", None, \"Choose a player. They draw 3 cards and lose 5 health.\"),\n",
    "    (1, \"Fold\", \"monster\", 0, \"short\", None, \"Choose a player. They gain 4 health and discard the top 2 cards of their deck.\"),\n",
    "    (3, \"Poker Face\", \"monster\", 2, \"short\", None, \"Deal 4 damage (to any player or long card).\"),\n",
    "    (3, \"Cheap Shot\", \"monster\", 2, \"short\", None, \"Deal 2 damage (to any player or long card). Draw a card.\"),\n",
    "    (1, \"The 'Ol Switcheroo\", \"monster\", 3, \"short\", None, \"The Hero and the Monster switch health.\"),\n",
    "    (2, \"Ultimatum\", \"monster\", 1, \"short\", None, \"Search your deck for any two cards you want with different names and reveal them. Your opponent chooses one of them. Put the chosen card into your hand, and shuffle the other back into your deck.\"),\n",
    "    (3, \"Peek\", \"monster\", 1, \"short\", None, \"Look at the top 2 cards of your deck and put one into your hand, and the other on the bottom of your deck.\"),\n",
    "]\n",
    "\n",
    "card_data = hero_card_data + monster_card_data\n",
    "\n",
    "num_cards = 0\n",
    "\n",
    "long_cards_vector = []  # List of the uids of just long cards\n",
    "\n",
    "uid = 0  # This thing is to get num_cards and the long_cards_vector. (Overlaps with build_deck in utils, but that's ok for now.)\n",
    "for i in range(len(card_data)):\n",
    "    quantity, name, owner, power_cost, card_type, health, card_text = card_data[i]\n",
    "    for j in range(quantity):\n",
    "        num_cards += 1\n",
    "        if card_type == \"long\":\n",
    "            long_cards_vector.append(uid)\n",
    "        uid += 1\n",
    "\n",
    "# Game phases. The strings can be edited to say anything and the game will run the same.\n",
    "PHASE_AWAITING_INPUT = \"Awaiting input.\"\n",
    "PHASE_PLAYING_SELECTED_CARD = \"Choosing how to play selected card.\"\n",
    "PHASE_VIEWING_CARD_INFO = \"Viewing card info.\"\n",
    "PHASE_SELECTING_GRAVEYARD_CARD = \"Choosing card from graveyard.\"\n",
    "PHASE_REORDERING_DECK_TOP3 = \"Reordering top 3 cards of deck. First card chosen goes on top, second card below that, and third is last.\"\n",
    "PHASE_SACRIFICING_LONG_CARD = \"Choosing a Noble Sacrifice.\"\n",
    "PHASE_DISCARDING_CARD_FROM_OPP_HAND = \"Looking at opponent's hand and discarding a card from it.\"\n",
    "PHASE_CHOOSING_GO_ALL_IN_TARGET = \"Choosing Go All In target.\"\n",
    "PHASE_CHOOSING_FOLD_TARGET = \"Choosing Fold target.\"\n",
    "PHASE_CHOOSING_POKER_FACE_TARGET = \"Choosing Poker Face target.\"\n",
    "PHASE_CHOOSING_CHEAP_SHOT_TARGET = \"Choosing Cheap Shot target.\"\n",
    "PHASE_CHOOSING_ULTIMATUM_CARD = \"Choosing an Ultimatum card from deck.\"\n",
    "PHASE_OPP_CHOOSING_FROM_ULTIMATUM = \"Opponent choosing from Ultimatum.\"\n",
    "PHASE_CHOOSING_FROM_DECK_TOP2 = \"Choosing from Peek.\"\n",
    "PHASE_HAND_FULL_DISCARDING_CARD = \"Hand full, discarding card.\"\n",
    "\n",
    "game_phases = [\n",
    "    PHASE_AWAITING_INPUT,\n",
    "    PHASE_PLAYING_SELECTED_CARD,\n",
    "    PHASE_VIEWING_CARD_INFO,\n",
    "    PHASE_SELECTING_GRAVEYARD_CARD,\n",
    "    PHASE_REORDERING_DECK_TOP3,\n",
    "    PHASE_SACRIFICING_LONG_CARD,\n",
    "    PHASE_DISCARDING_CARD_FROM_OPP_HAND,\n",
    "    PHASE_CHOOSING_GO_ALL_IN_TARGET,\n",
    "    PHASE_CHOOSING_FOLD_TARGET,\n",
    "    PHASE_CHOOSING_POKER_FACE_TARGET,\n",
    "    PHASE_CHOOSING_CHEAP_SHOT_TARGET,\n",
    "    PHASE_CHOOSING_ULTIMATUM_CARD,\n",
    "    PHASE_OPP_CHOOSING_FROM_ULTIMATUM,\n",
    "    PHASE_CHOOSING_FROM_DECK_TOP2,\n",
    "    PHASE_HAND_FULL_DISCARDING_CARD\n",
    "]\n",
    "\n",
    "ERROR_ENEMY_HAS_THE_SUN = \"Enemy has The Sun, you can't play more than one card per turn.\"\n",
    "ERROR_ENEMY_HAS_THE_MOON = \"Enemy has The Moon, you can't play power cards.\"\n",
    "ERROR_INVALID_SELECTION = \"Invalid selection.\"\n",
    "ERROR_CANT_PLAY_ANOTHER_POWER_CARD = \"You can't play another power card this turn.\"\n",
    "ERROR_NOT_ENOUGH_POWER = \"Not enough power to play this card.\"\n",
    "ERROR_NO_SACRIFICE = \"As an additional cost to play this card, you must sacrifice a long card.\"\n",
    "ERROR_MUST_PICK_DIFFERENT_CARD = \"Can't pick the same card twice.\"\n",
    "ERROR_MUST_HAVE_DIFFERENT_NAME = \"Card must have a different name.\"\n",
    "ERROR_NO_FURTHER_MOVES = \"No further moves available with this card.\"\n",
    "ERROR_COMPUTERS_CANT_DO = \"Computers can't do this action.\"  # Canceling and seeing card info are QOL features for people, not computers\n",
    "ERROR_ACTION_WITHELD_FROM_AI = \"This action is witheld from AIs since it would harm their strategy\"  # These are like automatic reflexes for the AI - things not to do that would harm \n",
    "\n",
    "num_actions = num_cards + 2  # +2 for end turn and cancel, computers can't cancel so for them it's num_cards + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7ca0f",
   "metadata": {},
   "source": [
    "## The Card Class\n",
    "\n",
    "The Card Class is the first of the four major classes for the game engine code: Card, Player, GameState, and Action. Since this is a turn-based game, the interactions are very cyclical, iterative, and recursive. Because of this, the code was very tricky to make, as any small mistake becomes amplified with every following turn. However, it now has (I think) no bugs (but you never know... bugs are pesky).\n",
    "\n",
    "Anyway, the Card Class is in charge of being a sort of storage container for card information, like the card's name (For example, \"Noble Sacrifice\"), card_id (there are 40 cards in the game), unique id, or uid (there are 18 non-duplicate cards), card type (short or long), and so on.\n",
    "\n",
    "There are also different Card subclasses for each of the different unique cards. These subclasses make changes to the GameState based on what the card does (for example, Power Trip gives you 2 power), and are called on by the Action class. The actual function that does this is the effect() function, which is different for every card since they have different effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c8cc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, name, card_id, uid, owner, card_type, power_cost, health, card_text):\n",
    "        # The smallest unit of gameplay.\n",
    "        self.name = name\n",
    "        self.card_id = card_id\n",
    "        self.uid = uid  # unique identifier (duplicates have different uids)\n",
    "        self.owner = owner  # \"hero\" or \"monster\"; can change by stealing with A Playful Pixie; this info will need to be shown to an AI\n",
    "        self.card_type = card_type  # \"short\" or \"long\"\n",
    "        self.power_cost = power_cost\n",
    "        self.health = health\n",
    "        self.starting_health = health  # Used to reset health after a card dies so it can be played again.\n",
    "        self.card_text = card_text  # Will be displayed\n",
    "\n",
    "    def effect(self, gs) -> None:\n",
    "        # Changes the game state based on the card's effect. \n",
    "        # Every effect has its own subclass.\n",
    "        raise NotImplementedError(\"Subclass must implement effect()\")\n",
    "\n",
    "class Awakening(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Awakening\")\n",
    "        flipped_power_cards = gs.me.power_cards\n",
    "        gs.me.power_cards = []  # reset power cards\n",
    "        for power_card in flipped_power_cards:\n",
    "            if power_card.card_type == \"short\":\n",
    "                gs.me.hand.append(power_card)\n",
    "            if power_card.card_type == \"long\":\n",
    "                gs.me.battlefield.append(power_card)\n",
    "\n",
    "class HealthyEating(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Healthy Eating\")\n",
    "        gs.me.draw()\n",
    "        gs.me.power_plays_left += 1\n",
    "\n",
    "# No subclasses for The Sun and The Moon\n",
    "\n",
    "class APLayfulPixie(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"A Playful Pixie effect triggered\")\n",
    "        if gs.opp.deck:\n",
    "            card = gs.opp.deck.pop(0)\n",
    "            card.owner = gs.me.name\n",
    "            gs.me.hand.append(card)\n",
    "\n",
    "class APearlescentDragon(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"A Pearlescent Dragon effect triggered\")\n",
    "        gs.opp.health -= 5\n",
    "        gs.me.health += 5\n",
    "\n",
    "class LastStand(Card):\n",
    "    # Needs to work with all graveyard conditions\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Last Stand\")\n",
    "        selected_cards = gs.cache[1:]  # If there are no selected cards, gs.cache[1:] returns an empty list\n",
    "        for card in selected_cards:  # These cards will be taken out of the graveyard and shuffled into the deck\n",
    "            gs.me.graveyard.remove(card)\n",
    "            gs.me.deck.append(card)\n",
    "        gs.me.shuffle()\n",
    "        gs.me.last_stand_buff = True\n",
    "        #print(\"Last Stand buff granted\")\n",
    "\n",
    "class Reconsider(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Reconsider\")\n",
    "        cards_in_new_order = gs.cache [1:]  # If there are fewer than 3 cards left in the deck, this will return that many cards\n",
    "        # Reinstate deck\n",
    "        gs.me.deck = gs.me.deck[3:]  # Removes top 3 cards of deck (or all if there's 3 or less cards remaining)\n",
    "        for card in reversed(cards_in_new_order):\n",
    "            gs.me.deck.insert(0, card)  # First card chosen goes first, last card chosen goes last\n",
    "\n",
    "class NobleSacrifice(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Noble Sacrifice\")\n",
    "        sacrifice = gs.cache[1]\n",
    "        discard = gs.cache[2]  # Can play when opp has no cards in hand, need to code this\n",
    "        sacrifice.health -= 10\n",
    "        if discard:  # If discard exists, discard it.\n",
    "            gs.opp.discard(discard)\n",
    "\n",
    "# For the buff to work properly, short_card_played_this_turn must be updated properly\n",
    "class MonstersPawn(Card):\n",
    "    def effect(self, gs):\n",
    "        if not gs.short_card_played_this_turn:\n",
    "            if gs.me.monsters_pawn_buff != True:\n",
    "                #print(\"Monster's Pawn buff granted\")  # Don't print this message if you already have the buff\n",
    "                ...\n",
    "            gs.me.monsters_pawn_buff = True\n",
    "        else:\n",
    "            gs.me.monsters_pawn_buff = False\n",
    "\n",
    "class PowerTrip(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Power Trip\")\n",
    "        gs.me.power += 2\n",
    "\n",
    "# No subclasses for Go All In and Fold\n",
    "\n",
    "class PokerFace(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Poker Face\")\n",
    "        # When Poker Face targets a player, that is handled in the action class.\n",
    "        target = gs.cache[1]\n",
    "        target.health -= 4\n",
    "\n",
    "class CheapShot(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Cheap Shot\")\n",
    "        # When Cheap Shot targets a player, that is handled in the action class.\n",
    "        target = gs.cache[1]\n",
    "        target.health -= 2\n",
    "        gs.me.draw()\n",
    "\n",
    "class TheOlSwitcheroo(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing The 'Ol Switcheroo\")\n",
    "        temp_health = gs.hero.health\n",
    "        gs.hero.health = gs.monster.health\n",
    "        gs.monster.health = temp_health\n",
    "\n",
    "class Ultimatum(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Ultimatum\")\n",
    "        # Note: current turn_priority should be the person playing the card (not the opp)\n",
    "        ultimatum = gs.cache[1:3]  # Note: this is a list with two cards\n",
    "        for card in ultimatum:\n",
    "            if card in gs.me.deck:\n",
    "                gs.me.deck.remove(card)  # remove ultimatum cards from deck\n",
    "        opp_selected_card = gs.cache[-1]\n",
    "        ultimatum.remove(opp_selected_card)\n",
    "        gs.me.hand.append(opp_selected_card)\n",
    "        gs.me.deck.append(ultimatum[0])  # Unselected card goes into deck\n",
    "        gs.me.shuffle()\n",
    "\n",
    "class Peek(Card):\n",
    "    def effect(self, gs):\n",
    "        #print(\"Playing Peek\")\n",
    "        deck_top2 = gs.me.deck[:2]  # copy top 2 cards of deck\n",
    "        gs.me.deck = gs.me.deck[2:]  # remove top 2 cards of deck\n",
    "        selected_card = gs.cache[1]\n",
    "        gs.me.hand.append(selected_card)  # put selected card into hand\n",
    "        deck_top2.remove(selected_card)\n",
    "        gs.me.deck.append(deck_top2[0])  # Put the other card on the bottom.\n",
    "        # Don't forget to get the index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055355b5",
   "metadata": {},
   "source": [
    "## The Player Class\n",
    "\n",
    "The Player Class is of two kinds: Hero and Monster. These are the two players, and the player types can be a computer that does random moves, a computer AI that is trained with reinforcement learning (a much tougher opponent), and a person controlled by a real human being. You can match any of these player types against each other.\n",
    "\n",
    "Each player has several 'zones':\n",
    "1. Their deck - the cards they draw from over the course of the game and get their starting hand from\n",
    "2. Their hand - the cards they are able to play (face-up or face-down)\n",
    "3. Their battlefield - cards they played which have persistent value\n",
    "4. Their graveyard - cards from the battlefield which have died, or cards they played from their hand that are discarded directly after use\n",
    "5. Their power cards - cards they played in order to get power, which is used to play face-up cards\n",
    "\n",
    "All of these zones are populated with various cards (from the Card class), that change throughout the game based on various actions.\n",
    "\n",
    "The Player Class also keeps track of some slightly less important information, like various buffs, who is going first, their power levels, health, and so on.\n",
    "\n",
    "The Player Class has various functions that are called on from the Action class with certain actions. That's what all the functions are for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "013ed570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name, deck, player_type=\"computer\"):\n",
    "        # Initializes a player with a name and a deck of cards.\n",
    "        self.name = name  # \"hero \" or \"monster\"\n",
    "        self.deck = deck\n",
    "        self.hand = []\n",
    "        self.battlefield = []\n",
    "        self.graveyard = []\n",
    "        self.health = 15  # Starting health\n",
    "        self.power_cards = []\n",
    "        self.power = 0  # Used to play cards face up, gained from playing cards face down.\n",
    "        self.power_plays_left = 1\n",
    "        self.power_plays_made_this_turn = 0\n",
    "        self.last_stand_buff = False\n",
    "        self.monsters_pawn_buff = False\n",
    "        self.going_first = False\n",
    "        self.player_type = player_type # \"person\" or \"computer\" - being a computer means being unable to cancel actions or view card info\n",
    "        self.action_number = 0\n",
    "\n",
    "    def start_turn(self, gs):\n",
    "        self.power_plays_left = 1\n",
    "        self.power_plays_made_this_turn = 0\n",
    "        self.power += len(self.power_cards)\n",
    "        self.draw()\n",
    "        for long_card in self.battlefield:\n",
    "            if long_card.name not in [\"The Moon\", \"The Sun\"]:\n",
    "                #print(\"Trying to trigger effect: \" + long_card.name)\n",
    "                long_card.effect(gs)\n",
    "                if long_card.name != \"Monster's Pawn\":\n",
    "                    gs.long_card_value += 0.08  # Gain long-term value for having long cards triggger. \n",
    "        if any(long_card.name == \"Monster's Pawn\" for long_card in self.battlefield):\n",
    "            gs.long_card_value += 0.08  # No value for having Monster's Pawn trigger more than once\n",
    "        if self.last_stand_buff:\n",
    "            self.last_stand_buff = False  # Set this to 0 at the start of one's own turn so that it can be active during opponent's turn\n",
    "            #print(\"Last Stand buff wore off\")\n",
    "\n",
    "    def end_turn(self, gs):\n",
    "        if gs.game_mode == 0:  # Reset power if game mode is normal. Else, let it carry over. Simple change.\n",
    "            self.power = 0\n",
    "        gs.pass_priority()\n",
    "\n",
    "    def draw(self, qty=1):\n",
    "        for i in range(qty):\n",
    "            # If there are no cards left in the deck, do not draw to prevent an error.\n",
    "            if self.deck:\n",
    "                card = self.deck.pop(0)\n",
    "                self.hand.append(card)\n",
    "\n",
    "    def mill(self, qty=1):  # Burns cards from the top of the deck\n",
    "        for i in range(qty):\n",
    "            if self.deck:\n",
    "                card = self.deck.pop(0)\n",
    "                self.graveyard.append(card)\n",
    "\n",
    "    def shuffle(self):\n",
    "        from random import shuffle\n",
    "        shuffle(self.deck)\n",
    "\n",
    "    def discard(self, card):\n",
    "        self.hand.remove(card)\n",
    "        self.graveyard.append(card)\n",
    "\n",
    "    # Playing a card face down\n",
    "    def play_power_card(self, gs, card):  \n",
    "        self.hand.remove(card)\n",
    "        self.power_cards.append(card)\n",
    "        self.power_plays_left -= 1\n",
    "        self.power_plays_made_this_turn += 1\n",
    "        self.power += 1\n",
    "\n",
    "    def pay_power_cost(self, gs, card):\n",
    "        #print(\"Paying power cost\")\n",
    "        if card.card_type == \"short\":\n",
    "            if self.monsters_pawn_buff == True:\n",
    "                #print(\"Monster's Pawn buff used\")\n",
    "                self.monsters_pawn_buff = False  # Monster's Pawn buff is used up, and don't have to pay power cost.\n",
    "            else:\n",
    "                self.power -= card.power_cost\n",
    "        if card.card_type == \"long\":\n",
    "            self.power -= card.power_cost\n",
    "\n",
    "    # Playing a card face up, either long or short.\n",
    "    def play_face_up(self, gs, card, no_effect=False):\n",
    "        # Pass no_effect to play_short_card (and not play_long_card) since only short cards have effects when you play them\n",
    "        #print(\"no more info needed\")\n",
    "        #print(\"playing card face up: \" + card.name)\n",
    "        if card.card_type == \"short\":\n",
    "            self.play_short_card(gs, card, no_effect)\n",
    "            gs.short_card_played_this_turn = True  # For Monster's Pawn\n",
    "        elif card.card_type == \"long\":\n",
    "            self.play_long_card(gs, card)\n",
    "        gs.card_played_this_turn = True  # For The Sun\n",
    "\n",
    "    def play_long_card(self, gs, card):\n",
    "        self.pay_power_cost(gs, card)\n",
    "        self.hand.remove(card)\n",
    "        self.battlefield.append(card)\n",
    "\n",
    "    def play_short_card(self, gs, card, no_effect=False):\n",
    "        self.pay_power_cost(gs, card)\n",
    "        self.hand.remove(card)\n",
    "        if not no_effect:  # This is for TargetHero and TargetMonster actions\n",
    "            card.effect(gs)\n",
    "        self.graveyard.append(card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8475eb8",
   "metadata": {},
   "source": [
    "## The GameState Class\n",
    "\n",
    "The GameState class contains all of the game data, including Hero and Monster and their everything (hands, decks, graveyards, power_cards, etc.). The GameState also keeps track of the game_phase and cache, which are needed to execute multi-step actions/cards. The GameState also checks for the winner, manages turn priority, checks for deaths on the battlefield, and is the ultimate source of feedback for the AI. Feedback is the only part of the AI code that you can't just grab off the shelf, but actually need to design with a bit of game understanding. There are four sources of feedback: \n",
    "1. Tempo - adressed in the Action class\n",
    "2. Killer combos - rewards for making cool plays using the cards Awakening and The 'OL Switcheroo\n",
    "3. Long card value - rewards for keeping your long cards alive\n",
    "4. Winning or losing the game - this is the largest and most important source of feedback\n",
    "\n",
    "Critically, the GameState also keeps track of who \"me\" and the \"opp\" is using the turn priority. These things are used all over the code, and just refer to which player has turn priority and is making actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac7b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, hero, monster, turn_priority=None, game_phase=PHASE_AWAITING_INPUT, cache=[], game_mode=0):\n",
    "        # Initializes the game state. Contains both players.\n",
    "        self.hero = hero\n",
    "        self.monster = monster\n",
    "        self.turn_priority = turn_priority  # Basically whose turn it is to take an action\n",
    "        self.game_phase = game_phase\n",
    "        self.cache = cache  # The cache, similar to Magic: The Gathering's stack\n",
    "        self.turn_number = 0\n",
    "        self.winner = None\n",
    "        self.game_mode = game_mode  # 0 is normal mode, 1 is \"Power Trip\" mode--power carries over at the end of the turn. Not often used (for future development).\n",
    "\n",
    "        self.card_played_this_turn = False  # Flag to track if any card has been played this turn\n",
    "        self.short_card_played_this_turn = False\n",
    "\n",
    "        self.tempo = 0  # Updated whenever a player does certain actions, cleared when the next action starts\n",
    "        self.killer_combo = 0  # Updated whenever a player pulls of an 'Ol Switcheroo combo or Awakening combo\n",
    "        self.long_card_value = 0  # Updated whenever a long card gets a start-of-turn trigger (updated directly in Player class, under start_turn)\n",
    "\n",
    "    @property\n",
    "    def me(self):\n",
    "        \"\"\"Returns the current player based on turn priority\"\"\"\n",
    "        return self.hero if self.turn_priority == \"hero\" else self.monster\n",
    "    \n",
    "    @property\n",
    "    def opp(self):\n",
    "        \"\"\"Returns the opponent based on turn priority\"\"\"\n",
    "        return self.monster if self.turn_priority == \"hero\" else self.hero\n",
    "\n",
    "    @property\n",
    "    def uncertainty(self):\n",
    "        # Uncertainty: This is a useful value for AIs to know - It corresponds to how much hidden information there is.\n",
    "        return len(self.opp.hand) + len(self.opp.deck) + len(self.opp.power_cards) + len(self.me.deck)\n",
    "    \n",
    "    def pass_priority(self):\n",
    "        \"\"\"Swaps turn priority between hero and monster.\"\"\"\n",
    "        self.turn_priority = \"monster\" if self.turn_priority == \"hero\" else \"hero\"\n",
    "\n",
    "    def turn_transition(self):\n",
    "        # Ends one player's turn and starts the other's\n",
    "        #print(f\"Ending {self.me.name}'s turn\")\n",
    "        self.me.end_turn(self)\n",
    "        # Note that turn priority changes here, so the next player is now the previous opponent.\n",
    "        self.turn_number += 1\n",
    "        self.card_played_this_turn = False\n",
    "        self.short_card_played_this_turn = False  # Make sure this is before start_turn so that Monster's Pawn can trigger. The order here is finnicky.\n",
    "        #print(f\"Starting {self.me.name}'s turn\")\n",
    "        self.me.start_turn(self)\n",
    "\n",
    "    # Check this after every action\n",
    "    def check_game_over(self) -> None:\n",
    "        \"\"\"Determine winner, if any.\"\"\"\n",
    "        monster_win = False\n",
    "        hero_win = False\n",
    "    \n",
    "        # Check Hero loss conditions\n",
    "        if len(self.hero.deck) == 0:\n",
    "            monster_win = True\n",
    "        if self.hero.health < 1:\n",
    "            if self.hero.last_stand_buff:\n",
    "                #print(\"Hero stayed alive using Last Stand buff\")\n",
    "                self.hero.health = 1  # Keeps Hero alive\n",
    "            else:\n",
    "                monster_win = True\n",
    "    \n",
    "        # Check Monster loss conditions\n",
    "        if self.monster.health < 1 or len(self.monster.deck) == 0:\n",
    "            hero_win = True\n",
    "    \n",
    "        # Determine final outcome\n",
    "        if monster_win and hero_win:\n",
    "            self.winner = \"tie\"\n",
    "        elif monster_win:\n",
    "            self.winner = \"monster\"\n",
    "        elif hero_win:\n",
    "            self.winner = \"hero\"\n",
    "\n",
    "    # Check this after every action\n",
    "    def check_long_card_deaths(self) -> None:\n",
    "        \"\"\"Check if any long cards on the battlefield have died.\"\"\"\n",
    "        for card in self.hero.battlefield[:]:\n",
    "            if card.health <= 0:\n",
    "                self.hero.battlefield.remove(card)\n",
    "                self.hero.graveyard.append(card)\n",
    "                card.health = card.starting_health  # Restore to full health after it is in the graveyard\n",
    "        for card in self.monster.battlefield[:]:\n",
    "            if card.health <= 0:\n",
    "                self.monster.battlefield.remove(card)\n",
    "                self.monster.graveyard.append(card)\n",
    "                card.health = card.starting_health\n",
    "\n",
    "    # Update this after every action\n",
    "    def update_pawn_buff(self):\n",
    "        for long_card in self.me.battlefield:\n",
    "            if long_card.name == \"Monster's Pawn\":\n",
    "                long_card.effect(self)  # This should trigger the buff\n",
    "\n",
    "        # Check if all Monster's Pawns have died on a given battlefield, and only then remove the buff\n",
    "        if not any(long_card.name == \"Monster's Pawn\" for long_card in self.me.battlefield):\n",
    "            self.me.monsters_pawn_buff = 0\n",
    "        if not any(long_card.name == \"Monster's Pawn\" for long_card in self.opp.battlefield):\n",
    "            self.opp.monsters_pawn_buff = 0\n",
    "\n",
    "    def long_term_reward(self, perspective):\n",
    "        # Reward shaping: assigning rewards to help shape learning outcomes. Sort of like school.\n",
    "        # Initialize rewards. Long-term rewards are only awarded at the end of the game.\n",
    "        hero_reward = 0\n",
    "        monster_reward = 0\n",
    "\n",
    "        # Winning/losing - Provide a reward for winning or losing the game - this is (and has to be) the largest reward/punishment out of any of these\n",
    "        self.check_game_over()  # This might not be necessary, but doesn't hurt to have it here.\n",
    "        if self.winner == \"hero\":\n",
    "            hero_reward += 1\n",
    "            monster_reward -= 1\n",
    "\n",
    "        elif self.winner == \"monster\":\n",
    "            monster_reward += 1\n",
    "            hero_reward -= 1\n",
    "\n",
    "        # Killer Cccombos: long-term rewards for pulling off difficult combos\n",
    "        hero_reward += self.killer_combo\n",
    "        monster_reward += self.killer_combo\n",
    "\n",
    "        # if self.killer_combo:\n",
    "        #     print(f\"KILLER COMBO {self.killer_combo:.2f} {self.me.name}\")\n",
    "\n",
    "        # Long card value: This needs to be a long-term reward so that the reward can reach back and reinforce the actions which led to its occurence (may have been several turns back)\n",
    "        hero_reward += self.long_card_value\n",
    "        monster_reward += self.long_card_value\n",
    "\n",
    "        # if self.long_card_value:\n",
    "        #     print(f\"Long card value: {self.long_card_value:.2f} {self.me.name}\")\n",
    "\n",
    "        # Return\n",
    "        if perspective == \"hero\":\n",
    "            return hero_reward\n",
    "        elif perspective == \"monster\":\n",
    "            return monster_reward\n",
    "\n",
    "    def short_term_reward(self, perspective):\n",
    "        # Short-term rewards are awarded during the entire game, before the game ends.\n",
    "        # These rewards need to be indicative of a win or loss. There must be some correlation between these things and winning or losing.\n",
    "        hero_reward = 0\n",
    "        monster_reward = 0\n",
    "\n",
    "        # These actions have a smaller gamma, so the reward does not reach back so far. It only reinforces the actions which came immediately before it.\n",
    "\n",
    "        # Tempo - get rewarded for taking certain actions, such as spending power. These points are awarded in the Actions section of the game engine code.\n",
    "        if self.turn_priority == \"hero\":\n",
    "            hero_reward += self.tempo / 8\n",
    "        else:\n",
    "            monster_reward += self.tempo / 8\n",
    "\n",
    "        # Return\n",
    "        if perspective == \"hero\":\n",
    "            return hero_reward\n",
    "        elif perspective == \"monster\":\n",
    "            return monster_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919a580",
   "metadata": {},
   "source": [
    "## The Action Class\n",
    "\n",
    "The Action class is in charge of directing changes to the Gamestate. There are multiple kinds of actions, and each has their own subclass. Each subclass (sub-action) has three parts:\n",
    "1. is_legal() - this determines whether a given action is legal and returns with a boolean: True/False, depending. The game engine would break if an illegal action was executed, so this prevents that.\n",
    "2. execute() - this directs the changes to the Gamestate by changing the game phase, adding cards to the cache, and executing cards face-up or face-down (among other things).\n",
    "3. get_tempo() - if is_legal is like the \"hard rules\" of what can and cannot be done, get_tempo is like the soft rules which define a fuzzy boundary for the AI to follow. It is a form of feedback - positive, negative, or neutral - for every action.\n",
    "\n",
    "All three of these functions are combined in the enact() function, which checks if an action is legal, gets the tempo, and then executes if it is legal. After a card is fully resolved, the turn ends, or an action is cancelled, the reset() function is called which empties the cache and resets the game_phase.\n",
    "\n",
    "I wrote is_legal() based on the rules of the game, and I wrote get_tempo based on *what I believe* is the optimal strategy for the game. (This means get_tempo might be slightly biased, which could lead to the AI being slightly biased.) Thankfully, it doesn't need to be perfect - just enough to help the AI learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de2bf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "class Action(object):\n",
    "    def __init__(self, gs, action_id):\n",
    "        # The largest unit of gameplay.\n",
    "        self.gs = gs\n",
    "        self.action_id = action_id  # Int from 0 to num_actions - 1. Corresponds to various action types. Each subclass is an action type.\n",
    "\n",
    "    def is_legal(self) -> Tuple[bool, Optional[str]]:\n",
    "        # Check if the action is legal. Returns a tuple of (is_legal, reason). If True, reason is None.\n",
    "        raise NotImplementedError(\"Subclass must implement is_legal()\")\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        # Execute the action, changing the gamestate.\n",
    "        raise NotImplementedError(\"Subclass must implement is_legal()\")\n",
    "\n",
    "    def get_tempo(self) -> int:\n",
    "        # Every single action has a tempo: positive, negative, or neutral. This tempo is used as feedback for the AI.\n",
    "        raise NotImplementedError(\"Subclass must implement get_tempo()\")\n",
    "\n",
    "    def enact(self) -> Tuple[bool, Optional[str]]:\n",
    "        # Enact = if it is legal, update some rewards, and then execute the action\n",
    "        legal, reason = self.is_legal()\n",
    "        # Reset tempo, killer_combo, and long_card_value, *before* the action is executed. This ensures the rewards only show up for a single time step.\n",
    "        self.gs.tempo = 0\n",
    "        self.gs.killer_combo = 0\n",
    "        self.gs.long_card_value = 0\n",
    "        self.gs.me.action_number += 1\n",
    "        if legal:\n",
    "            #print(\"Action is legal\")\n",
    "            # Get tempo - do before executing so that the effects don't change things first\n",
    "            self.get_tempo()  # Also updates Killer Combo (the last, long_card_value, is updated in start_turn() in the Player class)\n",
    "            try:\n",
    "                self.execute()\n",
    "            except:\n",
    "                print(\"The action had an error while executing.\")\n",
    "                print(f\"Action Class: {type(self).__name__}\")\n",
    "                print(f\"Action ID: {self.action_id} - Game Phase: {self.gs.game_phase}\")\n",
    "                print(f\"cache: {[card.name for card in self.gs.cache]}\")\n",
    "                raise TypeError\n",
    "            # Do these after every action\n",
    "            self.gs.update_pawn_buff()\n",
    "            self.gs.check_long_card_deaths()\n",
    "            self.gs.check_game_over()\n",
    "        else:\n",
    "            #print(\"Action is not legal\")\n",
    "            ...\n",
    "        return legal, reason\n",
    "\n",
    "    def predict_tempo(self) -> int:\n",
    "        # Returns what the tempo + killer combo is for an action - this does not permanently change the gamestate at all. This can be done anytime.\n",
    "        starting_tempo = self.gs.tempo\n",
    "        starting_killer_combo = self.gs.killer_combo\n",
    "        self.gs.tempo = 0  # Blank slate\n",
    "        tempo = self.get_tempo()  # Get tempo, which also changes the gamestate. Includes killer_combo.\n",
    "        self.gs.tempo = starting_tempo  # Revert changes\n",
    "        self.gs.killer_combo = starting_killer_combo\n",
    "        return tempo\n",
    "\n",
    "    # Reset the cache after resolving a given action.\n",
    "    def reset(self):\n",
    "        self.gs.cache = []\n",
    "        self.gs.game_phase = PHASE_AWAITING_INPUT\n",
    "        #print(\"Cleared cache\")\n",
    "\n",
    "# Does not represent a valid action, but is used to handle errors.\n",
    "class InvalidAction(Action):\n",
    "    def is_legal(self):\n",
    "        return False, ERROR_INVALID_SELECTION\n",
    "    \n",
    "    def execute(self):\n",
    "        raise ValueError(\"Invalid action taken\")\n",
    "\n",
    "    def get_tempo(self):\n",
    "        self.gs.tempo -= 1  # This will never happen, but it's fine.\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For cards that target the hero directly.\n",
    "class TargetHero(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "\n",
    "    def is_legal(self):\n",
    "        return True, None\n",
    "    \n",
    "    def execute(self):\n",
    "        if self.resolving_card.name == \"Go All In\":\n",
    "            #print(\"Playing Go All In\")\n",
    "            self.gs.hero.health -= 5\n",
    "            self.gs.hero.draw(3)\n",
    "        \n",
    "        elif self.resolving_card.name == \"Fold\":\n",
    "            #print(\"Playing Fold\")\n",
    "            self.gs.hero.health += 4\n",
    "            self.gs.hero.mill(2)\n",
    "        \n",
    "        elif self.resolving_card.name == \"Poker Face\":\n",
    "            #print(\"Playing Poker Face\")\n",
    "            self.gs.hero.health -= 4\n",
    "        \n",
    "        elif self.resolving_card.name == \"Cheap Shot\":\n",
    "            #print(\"Playing Cheap Shot\")\n",
    "            self.gs.hero.health -= 2\n",
    "            self.gs.me.draw()\n",
    "        # For all these cards:\n",
    "        self.gs.me.play_face_up(self.gs, self.resolving_card, no_effect=True)  # No effect since that was done just above\n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.resolving_card.name in [\"Poker Face\", \"Cheap Shot\"] and self.gs.me.name == \"monster\":\n",
    "            if any(card.name == \"The 'Ol Switcheroo\" for card in self.gs.me.hand):\n",
    "                self.gs.tempo += 0  # If you are targeting the Hero with these cards, you may want to wait until after you play The 'Ol Switcheroo, otherwise you don't get the benefit of the Switcheroo\n",
    "            else:\n",
    "                self.gs.tempo += 1  # Usually good to damage opponent unless Switcheroo is in your hand\n",
    "        elif self.resolving_card.name in [\"Poker Face\", \"Cheap Shot\"] and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo -= 1  # Don't target yourself\n",
    "        if self.resolving_card.name == \"Go All In\" and self.gs.me.name == \"monster\":\n",
    "            if any(card.name == \"The 'Ol Switcheroo\" for card in self.gs.me.hand):\n",
    "                self.gs.tempo += 0  # You may want to target yourself instead, but it's not bad to target the Hero here\n",
    "            else:\n",
    "                self.gs.tempo += 1  # It's good to go hard on opponent if no Switcheroo in hand\n",
    "        elif self.resolving_card.name == \"Go All In\" and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo -= 2  # Don't target yourself ever with this if you are the Hero\n",
    "        if self.resolving_card.name == \"Fold\" and self.gs.me.name == \"monster\":\n",
    "            if any(card.name == \"The 'Ol Switcheroo\" for card in self.gs.me.hand):\n",
    "                if self.gs.opp.health + 4 > self.gs.me.health:\n",
    "                    self.gs.tempo += 2  # Great play, since you can reverse the health gain (+4) with the Switcheroo\n",
    "                else:\n",
    "                    self.gs.tempo += 1\n",
    "            else:\n",
    "                self.gs.tempo += 0  # Not always great, since you heal the opponent, which makes a damage kill harder to accomplish\n",
    "        elif self.resolving_card.name == \"Fold\" and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo -= 1  # Don't target yourself with this\n",
    "        # These are for trying to teach the AI to clear out long cards:\n",
    "        if self.resolving_card.name == \"Poker Face\" and any(card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\", \"Monster's Pawn\"] for card in self.gs.opp.battlefield):\n",
    "            self.gs.tempo -= 1  # You should not target your opponent's face if your opponent has a threatening long card you could kill instead\n",
    "        elif self.resolving_card.name == \"Cheap Shot\" and any(card.name in [\"The Sun\", \"The Moon\"] for card in self.gs.opp.battlefield):\n",
    "            self.gs.tempo -= 1  # You should not target face if your opponent has a great target for it\n",
    "        if self.gs.opp.health <= 5 and self.resolving_card.name != \"Fold\" and self.gs.me.name == \"monster\":\n",
    "            self.gs.tempo += 1  # Bonus for going for lethal on Hero if you are the Monster\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For cards that target the monster directly. Mirrors TargetHero.\n",
    "class TargetMonster(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "\n",
    "    def is_legal(self):\n",
    "        return True, None\n",
    "    \n",
    "    def execute(self):\n",
    "        if self.resolving_card.name == \"Go All In\":\n",
    "            #print(\"Playing Go All In\")\n",
    "            self.gs.monster.health -= 5\n",
    "            self.gs.monster.draw(3)\n",
    "        \n",
    "        elif self.resolving_card.name == \"Fold\":\n",
    "            #print(\"Playing Fold\")\n",
    "            self.gs.monster.health += 4\n",
    "            self.gs.monster.mill(2)\n",
    "        \n",
    "        elif self.resolving_card.name == \"Poker Face\":\n",
    "            #print(\"Playing Poker Face\")\n",
    "            self.gs.monster.health -= 4\n",
    "        \n",
    "        elif self.resolving_card.name == \"Cheap Shot\":\n",
    "            #print(\"Playing Cheap Shot\")\n",
    "            self.gs.monster.health -= 2\n",
    "            self.gs.me.draw()\n",
    "        # For all these cards:\n",
    "        self.gs.me.play_face_up(self.gs, self.resolving_card, no_effect=True)  # No effect since that was done just above\n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.resolving_card.name in [\"Poker Face\", \"Cheap Shot\"] and self.gs.me.name == \"monster\":\n",
    "            self.gs.tempo -= 2  # You shouldn't damage yourself with these cards in 99.999% of situations\n",
    "        elif self.resolving_card.name in [\"Poker Face\", \"Cheap Shot\"] and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo += 1  # It's fine to damage the opponent with these when you are the Hero (so long as you have enough cards in deck if you are playing Cheap Shot)\n",
    "            if self.resolving_card.name == \"Cheap Shot\" and len(self.gs.me.deck) <= 4:\n",
    "                self.gs.tempo -= 1\n",
    "        if self.resolving_card.name == \"Go All In\" and self.gs.me.name == \"monster\":\n",
    "            if any(card.name == \"The 'Ol Switcheroo\" for card in self.gs.me.deck):\n",
    "                self.gs.tempo += 1  # It's fine to damage yourself with this card, since you draw 3 cards and can reverse the damage (later) with The 'Ol Switcheroo\n",
    "            elif any(card.name == \"The 'Ol Switcheroo\" for card in self.gs.me.hand):\n",
    "                self.gs.tempo += 2  # If you already have the other piece of the combo, you get an extra point\n",
    "            else:\n",
    "                self.gs.tempo += 0  # You probably shouldn't do this, but drawing 3 cards can be good\n",
    "        elif self.resolving_card.name == \"Go All In\" and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo += 2  # You should always target the Monster with this card if you have it and you are the Hero, since in almost all situations you won't have The 'Ol Switcheroo\n",
    "        if self.resolving_card.name == \"Fold\" and self.gs.me.name == \"monster\":\n",
    "            self.gs.tempo -= 1  # You shouldn't do this in 99% of situations\n",
    "        elif self.resolving_card.name == \"Fold\" and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo += 1  # Fine to do\n",
    "        # These are for trying to teach the AI to clear out long cards:\n",
    "        if self.resolving_card.name == \"Poker Face\" and any(card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\", \"Monster's Pawn\"] for card in self.gs.opp.battlefield):\n",
    "            self.gs.tempo -= 1  # You should not target your opponent's face if your opponent has a threatening long card you could kill instead\n",
    "        elif self.resolving_card.name == \"Cheap Shot\" and any(card.name in [\"The Sun\", \"The Moon\"] for card in self.gs.opp.battlefield):\n",
    "            self.gs.tempo -= 1  # You should not target face if your opponent has a great target for it\n",
    "        if self.gs.opp.health <= 5 and self.resolving_card.name != \"Fold\" and self.gs.me.name == \"hero\":\n",
    "            self.gs.tempo += 1  # Bonus for going for lethal on Monster if you are the Hero\n",
    "        return self.gs.tempo\n",
    "\n",
    "class GetCardInfo(Action):\n",
    "    def is_legal(self):\n",
    "        if self.gs.me.player_type.startswith(\"computer\"):\n",
    "            return False, ERROR_COMPUTERS_CANT_DO\n",
    "        return True, None\n",
    "    \n",
    "    def execute(self):\n",
    "        # Display card info\n",
    "        self.gs.game_phase = PHASE_VIEWING_CARD_INFO  # You can read card text and cancel\n",
    "\n",
    "    def get_tempo(self):\n",
    "        self.gs.tempo += 0  # Doesn't affect the game\n",
    "        return self.gs.tempo\n",
    "\n",
    "# To cancel the current action and go back.\n",
    "class Cancel(Action):\n",
    "    def is_legal(self):\n",
    "        if self.gs.me.player_type.startswith(\"computer\"):\n",
    "            return False, ERROR_COMPUTERS_CANT_DO\n",
    "        return True, None\n",
    "    \n",
    "    def execute(self):\n",
    "        self.reset()  # Just resets the cache. No gamestate changes; all that's lost is information.\n",
    "\n",
    "    def get_tempo(self):\n",
    "        self.gs.tempo += 0  # No effect on the game itself, hence 0 tempo\n",
    "        return self.gs.tempo\n",
    "\n",
    "# To end the turn. This also starts the next turn for the opponent.\n",
    "class EndTurn(Action):\n",
    "    def is_legal(self):\n",
    "        return True, None  # If the option is available, it is always legal.\n",
    "    \n",
    "    def execute(self):        \n",
    "        if len(self.gs.me.hand) > 5:  # check if hand is over maximum hand size (5)\n",
    "            self.gs.game_phase = PHASE_HAND_FULL_DISCARDING_CARD\n",
    "        else:\n",
    "            self.reset()\n",
    "            self.gs.turn_transition()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.gs.me.power:\n",
    "            self.gs.tempo -= 1  # You shouldn't end your turn without spending all your power\n",
    "        if len(self.gs.me.power_cards) < 3:\n",
    "            if self.gs.me.power_plays_left and len(self.gs.me.hand) != 0:\n",
    "                self.gs.tempo -= 1  # If you have less than 3 power cards and can play another power card, you shouldn't end the turn without playing the dang power card\n",
    "        if self.gs.opp.battlefield:\n",
    "            if any(card.name in [\"Poker Face\", \"Cheap Shot\"] for card in self.gs.me.hand) and self.gs.me.power >= 2:\n",
    "                self.gs.tempo -= 1  # You shouldn't end your turn without clearing the opponent's long cards if possible\n",
    "        if len(self.gs.opp.deck) == 1:\n",
    "            self.gs.tempo += 1  # This wins you the game\n",
    "        if len(self.gs.me.hand) > 5:\n",
    "            self.gs.tempo -= 2  # You shouldn't end your turn with hand size over the maximum limit. Very bad.\n",
    "        if sum(1 for card in self.gs.me.power_cards if card.card_type == \"long\") > 1 and self.gs.me.power >= 3:\n",
    "            self.gs.tempo -= 2  # If you have enough power to play Awakening and you have two or more long cards in your power cards, you shouldn't end the turn without playing it.\n",
    "        return self.gs.tempo\n",
    "\n",
    "# To select a card from a list (e.g., hand or battlefield).\n",
    "# After selecting a card from anywhere, it is added to the cache.\n",
    "# The bottom card in the cache, index [0], is always the one being played--the resolving card.\n",
    "class SelectFromHand(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.hand # Search this list for a card with a matching uid.\n",
    "        self.resolving_card = None # Will store selected card here\n",
    "\n",
    "    def future_moves_available(self, card) -> Tuple:\n",
    "        # This is just to simplify the game by showing fewer available moves that don't lead anywhere\n",
    "        # This enables not letting computers cancel their moves\n",
    "\n",
    "        # Proposed faster code (reverting the change rather than doing a deepcopy)\n",
    "        original_phase = self.gs.game_phase\n",
    "        self.gs.game_phase = PHASE_PLAYING_SELECTED_CARD\n",
    "        self.gs.cache.append(card)\n",
    "        test_face_up_action = PlayFaceUp(self.gs, 0)  # 0 = action_id; might not matter?\n",
    "        test_face_down_action = PlayFaceDown(self.gs, 1)\n",
    "        test_face_up_action_is_legal, reason = test_face_up_action.is_legal()\n",
    "        test_face_down_action_is_legal, reason = test_face_down_action.is_legal()\n",
    "        self.gs.cache.pop()\n",
    "        self.gs.game_phase = original_phase  # Revert the game phase back to the original state\n",
    "\n",
    "        # If the face up action is not legal and the face down action is not legal, then it should not be an option -> return False, ERROR_NO_FURTHER_MOVES\n",
    "        if not test_face_up_action_is_legal and not test_face_down_action_is_legal:\n",
    "            if self.gs.me.player_type.startswith(\"computer\"):  # Could make it an option for person players to enable this in the future\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def is_legal(self):\n",
    "        # Search card list for a card. If there is a match, action is legal.\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:  # Card's uid must match the action_id\n",
    "                # If card does not go anywhere, show as not legal\n",
    "                if self.future_moves_available(card) == False and self.gs.game_phase != PHASE_HAND_FULL_DISCARDING_CARD:  # Need game phase check since this action is also used for end of turn discard\n",
    "                    return False, ERROR_NO_FURTHER_MOVES\n",
    "                self.resolving_card = card\n",
    "                return True, None  # match found\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found, action is illegal.\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        self.gs.cache.append(self.resolving_card)  # Add selected card to the cache--always the [0] index\n",
    "        #print(f\"Added card '{self.resolving_card.name}' to cache via '{type(self).__name__}'\")\n",
    "        if self.gs.game_phase == PHASE_AWAITING_INPUT:\n",
    "            self.gs.game_phase = PHASE_PLAYING_SELECTED_CARD\n",
    "        \n",
    "        elif self.gs.game_phase == PHASE_HAND_FULL_DISCARDING_CARD:\n",
    "            self.gs.me.discard(self.resolving_card)\n",
    "            #print(\"Discarded card: \" + self.resolving_card.name)\n",
    "            if len(self.gs.me.hand) <= 5:  # 5 = max hand size\n",
    "                self.reset()\n",
    "                self.gs.turn_transition()\n",
    "            # Else, do nothing -- player has to keep discarding cards until they are below the maximum\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.gs.game_phase == PHASE_HAND_FULL_DISCARDING_CARD:\n",
    "            if self.resolving_card.name in [\"Go All In\", \"The 'Ol Switcheroo\", \"A Pearlescent Dragon\", \"A Playful Pixie\", \"Poker Face\", \"Last Stand\"]:\n",
    "                self.gs.tempo -= 1  # Don't discard these.\n",
    "            if self.resolving_card.name in [\"The 'Ol Switcheroo\", \"Last Stand\"]:\n",
    "                self.gs.tempo -= 1  # Especially not these, since they can't be replaced\n",
    "        # These are for trying to teach the AI to clear out long cards:\n",
    "        if any(card.name == \"Poker Face\" for card in self.gs.me.hand) and any(card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\"] for card in self.gs.opp.battlefield) and self.gs.me.power >= 2:\n",
    "            if self.resolving_card.name != \"Poker Face\":\n",
    "                self.gs.tempo -= 2  # You should pick Poker Face (to play face-up) if it is in your hand, you have the power, and your opponent has a threatening long card\n",
    "        if any(card.name == \"Cheap Shot\" for card in self.gs.me.hand) and any(card.name in [\"The Sun\", \"The Moon\"] for card in self.gs.opp.battlefield) and self.gs.me.power >= 2:\n",
    "            if self.resolving_card.name != \"Cheap Shot\":\n",
    "                self.gs.tempo -= 2  # You should pick Cheap Shot (to play face-up) if it is in your hand, you have the power, and your opponent has a great target for it\n",
    "        # This is for trying to encourage playing Awakening:\n",
    "        if sum(1 for card in self.gs.me.power_cards if card.card_type == \"long\") > 1 and self.gs.me.power >= 3:\n",
    "            if self.resolving_card.name != \"Awakening\":\n",
    "                self.gs.tempo -= 1  # If you have enough power to play Awakening and you have two or more long cards in your power cards, you shouldn't end the turn without playing it.\n",
    "        return self.gs.tempo\n",
    "\n",
    "# This time, the card list being searched is both battlefields.\n",
    "# For Poker Face and Cheap Shot\n",
    "class SelectFromBattlefield(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.battlefield + self.gs.opp.battlefield\n",
    "        self.target = None  # In this case, a target long card\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "\n",
    "    def is_legal(self):\n",
    "        # Search card list for a card with a matching uid\n",
    "        for long_card in self.card_list:\n",
    "            if long_card.uid == self.action_id:\n",
    "                self.target = long_card\n",
    "                if self.gs.me.player_type.startswith(\"computer_ai\"):\n",
    "                    if self.target in self.gs.me.battlefield:\n",
    "                        return False, ERROR_ACTION_WITHELD_FROM_AI  # The AI should never, ever target their own long card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "    \n",
    "    def execute(self) -> None:\n",
    "        self.gs.cache.append(self.target)  # Add selected target to the cache\n",
    "        #print(f\"Added card '{self.target.name}' to cache via '{type(self).__name__}'\")\n",
    "        self.gs.me.play_face_up(self.gs, self.resolving_card)  \n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.target in self.gs.me.battlefield:\n",
    "            self.gs.tempo -= 2 # You shouldn't target your own long cards, ever\n",
    "        if self.target in self.gs.opp.battlefield:\n",
    "            self.gs.tempo += 1  # You *should* target the opponent's long cards\n",
    "        if self.resolving_card.name == \"Poker Face\" and self.target.name in [\"The Sun\", \"The Moon\"]:\n",
    "            self.gs.tempo -= 0  # This is overkill, but still okay in some situations\n",
    "        if self.resolving_card.name == \"Cheap Shot\" and self.target.name in [\"The Sun\", \"The Moon\"]:\n",
    "            self.gs.tempo += 2  # This is not overkill, Cheap Shot is meant to kill these cards\n",
    "        if self.resolving_card.name == \"Poker Face\" and self.target.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\", \"Monster's Pawn\"]:\n",
    "            self.gs.tempo += 2  # Similarly, Poker Face is definitely meant to kill these cards\n",
    "        if self.resolving_card.name == \"Cheap Shot\" and self.target.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\"]:\n",
    "            self.gs.tempo += 1  # Decent play, since you can hit it with another Cheap Shot later to kill it, but that doesn't always happen. Hard to say whether this is neutral or positive tempo.\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For Noble Sacrifice\n",
    "class SelectFromOwnBattlefield(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.battlefield\n",
    "        self.sacrifice = None  # long card to be sacrificed as payment for Noble Sacrifice\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # Noble Sacrifice is on the bottom of the stack\n",
    "\n",
    "    def is_legal(self):\n",
    "        # Search battlefield for a sacrifice\n",
    "        for long_card in self.card_list:\n",
    "            if long_card.uid == self.action_id:\n",
    "                self.sacrifice = long_card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "\n",
    "    def execute(self):\n",
    "        self.gs.cache.append(self.sacrifice)  # Add selected sacrifice to the cache\n",
    "        #print(f\"Added card '{self.sacrifice.name}' to cache via '{type(self).__name__}'\")\n",
    "        # If enemy hand is empty, card can still be played. Just does nothing.\n",
    "        if len(self.gs.opp.hand) == 0:\n",
    "            #print(\"Nothing to discard\")\n",
    "            self.gs.cache.append(None)  # This will be tackled in card class.\n",
    "            #print(f\"Added None to cache via '{type(self).__name__}'\")\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "            self.reset()        \n",
    "        else:\n",
    "            self.gs.game_phase = PHASE_DISCARDING_CARD_FROM_OPP_HAND\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.sacrifice.name in [\"The Sun\", \"The Moon\"]:\n",
    "            self.gs.tempo += 1  # These are good cards to sacrifice, since they don't cost much\n",
    "        elif self.sacrifice.name == \"A Pearlescent Dragon\":\n",
    "            self.gs.tempo -= 1  # This is not a good sacrifice, since it is powerful, but it has a disadvantage since it weakens you to Switcheroo\n",
    "        elif self.sacrifice.name == \"A Playful Pixie\":\n",
    "            self.gs.tempo -= 2  # This is the worst card to sacrifice, since it has no downsides\n",
    "        if self.sacrifice.name in [\"A Pearlescent Dragon\", \"A Playful Pixie\"] and any(long_card.name in [\"The Sun\", \"The Moon\"] for long_card in self.gs.me.battlefield):\n",
    "            self.gs.tempo -= 1  # If you have The Sun or The Moon, you shouldn't sacrifice a stronger card\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For Noble Sacrifice\n",
    "class SelectFromOppHand(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.opp.hand\n",
    "        self.discard = None  # enemy card to discard\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # Noble Sacrifice is on the bottom of the stack/cache\n",
    "\n",
    "    def is_legal(self):\n",
    "        # Search opp hand for discard\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:\n",
    "                self.discard = card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "    \n",
    "    def execute(self):\n",
    "        self.gs.cache.append(self.discard)  # Append discard to the cache for next step.\n",
    "        #print(f\"Added card '{self.discard.name}' to cache via '{type(self).__name__}'\")\n",
    "        self.gs.me.play_face_up(self.gs, self.resolving_card)  \n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        # This needs a little work, to reward the best available discards, not just the best discards in general\n",
    "        if self.discard.name in [\"Peek\", \"Ultimatum\", \"Power Trip\"]:\n",
    "            self.gs.tempo -= 1  # These are not good cards to discard, since they are weak\n",
    "        elif self.discard.name in [\"Go All In\", \"Fold\", \"Poker Face\", \"The 'Ol Switcheroo\"]:\n",
    "            self.gs.tempo += 1  # These are good cards to remove, especially Go All In and The 'Ol Switcheroo:\n",
    "            if self.discard.name in [\"Go All In\", \"The 'Ol Switcheroo\"]:\n",
    "                self.gs.tempo += 1\n",
    "        else:\n",
    "            self.gs.tempo += 0  # All other discards are neutral\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For Peek\n",
    "class SelectFromDeckTop2(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.deck[:2]\n",
    "        self.selected_card = None  # card to put into hand\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # Noble Sacrifice is on the bottom of the stack/cache\n",
    "\n",
    "    def is_legal(self):\n",
    "        # choose card from top2\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:\n",
    "                self.selected_card = card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "    \n",
    "    def execute(self):\n",
    "        if len(self.gs.me.deck) == 1:  # You can play this with one card left, but\n",
    "            self.gs.me.draw()  # you just lose\n",
    "            self.reset()  # Make sure to reset so the GS is still valid after the game\n",
    "        else:\n",
    "            self.gs.cache.append(self.selected_card)  # Append selected card to the cache for next step.\n",
    "            #print(f\"Added card '{self.selected_card.name}' to cache via '{type(self).__name__}'\")\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)  \n",
    "            self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.selected_card.name in [\"Go All In\", \"Fold\", \"Poker Face\", \"The 'Ol Switcheroo\", \"Cheap Shot\"]:\n",
    "            self.gs.tempo += 1  # These are high priority cards, especially Go All In and The 'Ol Switcheroo\n",
    "            if self.selected_card.name in [\"Go All In\", \"The 'Ol Switcheroo\"]:\n",
    "                self.gs.tempo += 1\n",
    "        else:\n",
    "            self.gs.tempo += 0\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For Last Stand\n",
    "class SelectFromGraveyard(Action):\n",
    "    # Need to iterate this action class up to three times in order to get information for Last Stand to resolve\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.graveyard\n",
    "        self.selected_card = None  # card to shuffle into deck\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # In this case, Last Stand\n",
    "        \n",
    "        self.previously_selected_cards = self.gs.cache[1:]\n",
    "\n",
    "    def is_legal(self):\n",
    "        # choose card from graveyard that *hasn't already been chosen*\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:  # Test via uid\n",
    "                if card in self.previously_selected_cards:\n",
    "                    return False, ERROR_MUST_PICK_DIFFERENT_CARD  # Can't pick the same card twice\n",
    "                else:\n",
    "                    self.selected_card = card\n",
    "                    return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "\n",
    "    def execute(self):\n",
    "        # Need escape condition, else this action will loop over and over\n",
    "        self.gs.cache.append(self.selected_card)  # Append selected card to the cache until cache has 3 cards (not including resolving card)\n",
    "        #print(f\"Added card '{self.selected_card.name}' to cache via '{type(self).__name__}'\")\n",
    "        num_selected_cards = len(self.gs.cache[1:])  # This information is from after the selected card has been added in this step to the cache, unlike the legal check\n",
    "        if num_selected_cards == 3:  # Full graveyard condition - can nerf to 2\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "            self.reset()\n",
    "        elif num_selected_cards == len(self.gs.me.graveyard):  # Small graveyard condition\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "            self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        # Rewards to trigger for card selections:\n",
    "        if self.selected_card.name in [\"The Sun\", \"A Playful Pixie\", \"A Pearlescent Dragon\", \"Noble Sacrifice\"]:\n",
    "            self.gs.tempo += 1  # These are good targets to shuffle back into the deck\n",
    "        elif self.selected_card.name in [\"Healthy Eating\", \"Reconsider\"]:\n",
    "            self.gs.tempo -= 1  # Don't do this since you usually don't need more of these and Healthy Eating draws a card, reducing deck size which is not good for the late game\n",
    "        else:\n",
    "            self.gs.tempo += 0  # All other cards are neutral\n",
    "        return self.gs.tempo\n",
    "\n",
    "# For Ultimatum\n",
    "class SelectFromDeck(Action):\n",
    "    # choose two cards from deck with *different names*\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.deck\n",
    "        self.selected_card = None  # Will update if found\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # Ultimatum\n",
    "\n",
    "        self.previously_selected_card = None\n",
    "        if len(self.gs.cache) > 1:\n",
    "            self.previously_selected_card = self.gs.cache[1]  \n",
    "        # There are only two cards in an ultimatum, so having only one variable for prev selected cards is fine.\n",
    "\n",
    "        # Need to test if the deck actually has cards with different names\n",
    "        first_name = self.card_list[0].name\n",
    "        self.deck_has_different_names = any(card.name != first_name for card in self.card_list[1:])\n",
    "\n",
    "    def is_legal(self):\n",
    "        # This is essentially a stricter test in addition to unique id\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:\n",
    "                if self.previously_selected_card:\n",
    "                    if card.name == self.previously_selected_card.name:\n",
    "                        # Edge case: deck has two cards left, both with the same name. Allow it.\n",
    "                        if self.deck_has_different_names:\n",
    "                            return False, ERROR_MUST_HAVE_DIFFERENT_NAME  # Card must have different name, but can be another \"Ultimatum\"\n",
    "                self.selected_card = card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "\n",
    "    def execute(self):\n",
    "        self.gs.cache.append(self.selected_card)\n",
    "        #print(f\"Added card '{self.selected_card.name}' to cache via '{type(self).__name__}'\")\n",
    "        num_selected_cards = len(self.gs.cache[1:])  # !Again, this must be calculated again after appending the card.\n",
    "        if num_selected_cards == 2:  # Need two cards for an ultimatum\n",
    "            self.gs.game_phase = PHASE_OPP_CHOOSING_FROM_ULTIMATUM\n",
    "            self.gs.pass_priority()  # Pass priority so opponent can choose from established ultimatum (then pass it back)\n",
    "            #print(\"Passed priority\")\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.selected_card.name in [\"Go All In\", \"Fold\", \"Poker Face\", \"The 'Ol Switcheroo\", \"A Playful Pixie\", \"A Pearlescent Dragon\", \"Last Stand\", \"Noble Sacrifice\"]:\n",
    "            self.gs.tempo += 1  # These are high priority cards, especially Go All In and The 'Ol Switcheroo for Monster and Pixie and Dragon for Hero\n",
    "            if self.selected_card.name in [\"Go All In\", \"The 'Ol Switcheroo\", \"A Playful Pixie\", \"A Pearlescent Dragon\"]:\n",
    "                self.gs.tempo += 1\n",
    "        else:\n",
    "            self.gs.tempo += 0  # All other cards are not good, but not really bad either. You shouldn't be punished for not picking the above cards if they are not available.\n",
    "        return self.gs.tempo\n",
    "\n",
    "class SelectFromUltimatum(Action):\n",
    "    # Opp has priority over this step, which is the opposite of every other card in the game.\n",
    "    # Only with Ultimatum can the opponent make a decision during your turn.\n",
    "    # Just need to pass back priority after this.\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        ultimatum = self.gs.cache[1:]  # This is the two cards chosen during the previous two game phases\n",
    "        self.card_list = ultimatum  # Opponent searches the ultimatum\n",
    "        self.selected_card = None  # Update if found\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "\n",
    "    def is_legal(self):\n",
    "        # Similar to other searches\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:  # Test via uid\n",
    "                self.selected_card = card\n",
    "                return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "    \n",
    "    def execute(self):\n",
    "        self.gs.cache.append(self.selected_card)  # Add to cache\n",
    "        #print(f\"Added card '{self.selected_card.name}' to cache via '{type(self).__name__}'\")\n",
    "        self.gs.pass_priority()  # Pass it back before the card is handled in the card subclass\n",
    "        #print(\"Passed priority back\")\n",
    "        self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.selected_card.name in [\"Go All In\", \"Fold\", \"Poker Face\", \"The 'Ol Switcheroo\", \"A Playful Pixie\", \"A Pearlescent Dragon\", \"Last Stand\", \"Noble Sacrifice\"]:\n",
    "            self.gs.tempo -= 1  # These are high priority cards, especially Go All In and The 'Ol Switcheroo for M and yeah like above. The selected card goes into their hand.\n",
    "            if self.selected_card.name in [\"Go All In\", \"The 'Ol Switcheroo\", \"A Playful Pixie\", \"A Pearlescent Dragon\"]:\n",
    "                self.gs.tempo -= 1\n",
    "        else:\n",
    "            self.gs.tempo += 0\n",
    "        return self.gs.tempo\n",
    "\n",
    "# Reconsider\n",
    "class SelectFromDeckTop3(Action):\n",
    "    # Mimics Last Stand\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.card_list = self.gs.me.deck[:3]  # Top 3 cards of deck, mimics Peek\n",
    "        self.selected_card = None  # card to rearrange\n",
    "\n",
    "        self.resolving_card = self.gs.cache[0]  # In this case, Reconsider\n",
    "        \n",
    "        self.previously_selected_cards = self.gs.cache[1:]\n",
    "\n",
    "    def is_legal(self):\n",
    "        for card in self.card_list:\n",
    "            if card.uid == self.action_id:  # Test via uid\n",
    "                if card in self.previously_selected_cards:\n",
    "                    return False, ERROR_MUST_PICK_DIFFERENT_CARD  # Can't pick the same card twice\n",
    "                else:\n",
    "                    self.selected_card = card\n",
    "                    return True, None\n",
    "        return False, ERROR_INVALID_SELECTION  # If no match found\n",
    "    \n",
    "    def execute(self):\n",
    "        self.gs.cache.append(self.selected_card)  # Append selected card to the cache until cache has 3 cards (not including resolving card)\n",
    "        #print(f\"Added card '{self.selected_card.name}' to cache via '{type(self).__name__}'\")\n",
    "        num_selected_cards = len(self.gs.cache[1:])  # This information is from after the selected card has been added in this step to the cache, unlike the legal check\n",
    "        if num_selected_cards == 3:  # Full deck condition\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "            self.reset()\n",
    "        elif num_selected_cards == len(self.gs.me.deck):  # Small deck condition\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "            self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.selected_card.card_type == \"long\":  # It's generally good to move long cards to the top, especially Dragon and Pixie\n",
    "            self.gs.tempo += 1\n",
    "            if self.selected_card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\"]:\n",
    "                self.gs.tempo += 1\n",
    "        self.gs.tempo += 0  # This is a very low tempo card, but maybe certain choices should be rewarded\n",
    "        return self.gs.tempo\n",
    "\n",
    "# To play a long or short card face up.\n",
    "class PlayFaceUp(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "\n",
    "    def is_legal(self) -> Tuple[bool, Optional[str]]:\n",
    "        has_enough_power = self.resolving_card.power_cost <= self.gs.me.power\n",
    "        has_free_short_card = self.gs.me.monsters_pawn_buff\n",
    "        \n",
    "        if any(long_card.name == \"The Sun\" for long_card in self.gs.opp.battlefield) and self.gs.card_played_this_turn:\n",
    "            return False, ERROR_ENEMY_HAS_THE_SUN  # If enemy has the sun, can't play 2 cards in a turn\n",
    "        \n",
    "        elif self.resolving_card.card_type == \"short\":\n",
    "            if self.resolving_card.name == \"Noble Sacrifice\":\n",
    "                if not self.gs.me.battlefield:\n",
    "                    return False, ERROR_NO_SACRIFICE  # Must have available sacrifice\n",
    "                if not self.gs.opp.hand and self.gs.me.player_type.startswith(\"computer_ai\"):\n",
    "                    return False, ERROR_ACTION_WITHELD_FROM_AI  # Helping out the AI\n",
    "            if self.gs.me.player_type.startswith(\"computer_ai\"):\n",
    "                if self.resolving_card.name == \"The 'Ol Switcheroo\" and self.gs.me.health >= self.gs.opp.health:\n",
    "                    return False, ERROR_ACTION_WITHELD_FROM_AI  # Helping out the AI, you would never play Switcheroo if your opponent would benefit from it\n",
    "                if self.resolving_card.name == \"Awakening\" and not any(card for card in self.gs.me.power_cards if card.card_type == \"long\"):\n",
    "                    return False, ERROR_ACTION_WITHELD_FROM_AI  # The AI should never play Awakening if there aren't any long cards to flip\n",
    "            if has_free_short_card:\n",
    "                #print(\"Free short card available\")\n",
    "                return True, None  # Can play a free short card thanks to Monster's Pawn\n",
    "            elif not has_enough_power:\n",
    "                return False, ERROR_NOT_ENOUGH_POWER\n",
    "        \n",
    "        elif self.resolving_card.card_type == \"long\":\n",
    "            if not has_enough_power:\n",
    "                return False, ERROR_NOT_ENOUGH_POWER\n",
    "        return True, None\n",
    "\n",
    "    # Different game phases follow depending on the card.\n",
    "    def execute(self) -> None:\n",
    "        #print(\"Gathering info needed to play card face up\")\n",
    "        # These cards require extra info to play, which takes extra game phases\n",
    "        if self.resolving_card.name == \"Last Stand\":\n",
    "            if len(self.gs.me.graveyard) == 0:  # Can still play the card with no graveyard, just does nothing except grant the buff\n",
    "                # Empty graveyard condition\n",
    "                self.gs.me.play_face_up(self.gs, self.resolving_card)\n",
    "                self.reset()\n",
    "            else:\n",
    "                self.gs.game_phase = PHASE_SELECTING_GRAVEYARD_CARD\n",
    "        elif self.resolving_card.name == \"Reconsider\":\n",
    "            self.gs.game_phase = PHASE_REORDERING_DECK_TOP3\n",
    "        elif self.resolving_card.name == \"Noble Sacrifice\":\n",
    "            self.gs.game_phase = PHASE_SACRIFICING_LONG_CARD\n",
    "        elif self.resolving_card.name == \"Go All In\":\n",
    "            self.gs.game_phase = PHASE_CHOOSING_GO_ALL_IN_TARGET\n",
    "        elif self.resolving_card.name == \"Fold\":\n",
    "            self.gs.game_phase = PHASE_CHOOSING_FOLD_TARGET\n",
    "        elif self.resolving_card.name == \"Poker Face\":\n",
    "            self.gs.game_phase = PHASE_CHOOSING_POKER_FACE_TARGET\n",
    "        elif self.resolving_card.name == \"Cheap Shot\":\n",
    "            self.gs.game_phase = PHASE_CHOOSING_CHEAP_SHOT_TARGET\n",
    "        elif self.resolving_card.name == \"Ultimatum\":\n",
    "            if len(self.gs.me.deck) == 1:  # You can play this with one card left, but\n",
    "                self.gs.me.draw()  # you just lose\n",
    "            else: \n",
    "                self.gs.game_phase = PHASE_CHOOSING_ULTIMATUM_CARD\n",
    "        elif self.resolving_card.name == \"Peek\":\n",
    "            self.gs.game_phase = PHASE_CHOOSING_FROM_DECK_TOP2\n",
    "        # Play the card normally if no extra info needed:\n",
    "        else:\n",
    "            #print(\"No info needed\")\n",
    "            self.gs.me.play_face_up(self.gs, self.resolving_card)  \n",
    "            self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        # Here are the benefits for playing cards face up in specific ways\n",
    "        # KILLER COMBOS: The 'Ol Switcheroo and Awakening - long lasting rewards that stretch back in time\n",
    "        if self.resolving_card.name == \"The 'Ol Switcheroo\":\n",
    "            health_difference = self.gs.opp.health - self.gs.me.health  # The bigger the health difference, the better\n",
    "            upper_bound = 15  # 15 health is a reasonable best play to optimize for\n",
    "            if health_difference <= upper_bound:\n",
    "                self.gs.killer_combo += health_difference / upper_bound\n",
    "            elif health_difference > upper_bound:\n",
    "                self.gs.killer_combo += upper_bound / upper_bound  # +1 - worth as much as winning the game, actually\n",
    "        elif self.resolving_card.name == \"Awakening\":\n",
    "            cumulative_power = sum(power_card.power_cost for power_card in self.gs.me.power_cards if power_card.card_type == \"long\")\n",
    "            upper_bound = 14  # 14 power (The Sun + The Moon + A Playful Pixie + A Pearlescent Dragon; the complement) is a reasonable best play to optimize for\n",
    "            if cumulative_power:  # This is how you get to do the combo and you can maximize value by flipping many long cards at once\n",
    "                if cumulative_power <= upper_bound:\n",
    "                    self.gs.killer_combo += cumulative_power / upper_bound\n",
    "                elif cumulative_power > upper_bound:\n",
    "                    self.gs.killer_combo += upper_bound / upper_bound  # 1\n",
    "                self.gs.killer_combo /= 4  # Although there are three copies of Awakening but only one copy of The 'Ol Switcheroo, the total maximum reward will be +0.5\n",
    "                self.gs.killer_combo += 0.25  # Flat reward since the AI is holding onto Awakening too much\n",
    "            else:\n",
    "                self.gs.tempo -= 2  # Don't play if you don't have any long cards face down. Bad idea.\n",
    "        elif self.resolving_card.name == \"Healthy Eating\":\n",
    "            self.gs.tempo += 1  # Get tempo for drawing a card\n",
    "            if len(self.gs.me.deck) > 10:\n",
    "                self.gs.tempo += 1  # Good card for early game\n",
    "            elif len(self.gs.me.deck) <= 5:\n",
    "                self.gs.tempo -= 1  # Bad to play late-game, since it kills your deck a little bit\n",
    "            else:\n",
    "                self.gs.tempo += 0  # Mid-game play is neutral.\n",
    "        elif self.resolving_card.name == \"Power Trip\":\n",
    "            self.gs.tempo += 1  # High tempo card\n",
    "        elif self.resolving_card.name == \"The Sun\" and any(long_card.name == \"Monster's Pawn\" for long_card in self.gs.opp.battlefield):\n",
    "            self.gs.tempo += 1  # This is a counter to Monster's Pawn\n",
    "        elif self.resolving_card.name == \"The Moon\" and len(self.gs.opp.power_cards) == 2:\n",
    "            self.gs.tempo += 1  # This is a great move since they have to kill your The Moon before they can play their powerhouse 3-cost cards\n",
    "        elif self.resolving_card.name == \"Monster's Pawn\" and any(long_card.name == \"Monster's Pawn\" for long_card in self.gs.me.battlefield):\n",
    "            self.gs.tempo -= 2  # There is no benefit to having multiple of these\n",
    "        elif self.resolving_card.name == \"Last Stand\":\n",
    "            # Rewards to trigger on first play (the rewards for this are kind of harsh because there's a right and wrong way to play it):\n",
    "            # Rewards based on health:\n",
    "            if self.gs.me.health <= 5:\n",
    "                self.gs.tempo += 1  # You should play Last Stand if you are low on health, since it can save you from damage.\n",
    "            elif self.gs.me.health > 10:\n",
    "                self.gs.tempo -= 1  # Not a good move if not. Try to save it for when it can save your life.\n",
    "            else:\n",
    "                self.gs.tempo += 0  # Neutral if you are between 5 and 10 health. You could go lower but you risk dying.\n",
    "            # Rewards based on deck size:\n",
    "            if len(self.gs.me.deck) <= 5:\n",
    "                self.gs.tempo += 1  # Good value, since you have a lot of cards to choose from at this point and it can save you from mill.\n",
    "            elif len(self.gs.me.deck) > 10:\n",
    "                self.gs.tempo -= 1  # Try to save it for late game.\n",
    "            else:\n",
    "                self.gs.tempo += 0  # Mid-game play is alright but not ideal.\n",
    "        if self.resolving_card.card_type == \"long\":\n",
    "            self.gs.tempo += 1  # You get tempo for playing long cards\n",
    "        if self.resolving_card.power_cost >= 3:\n",
    "            self.gs.tempo += 1  # Playing more expensive cards yields higher tempo\n",
    "        return self.gs.tempo + self.gs.killer_combo  # Only get_tempo() that includes both\n",
    "\n",
    "# To play a power card.\n",
    "class PlayFaceDown(Action):\n",
    "    def __init__(self, gs, action_id):\n",
    "        super().__init__(gs, action_id)\n",
    "        self.resolving_card = self.gs.cache[0]\n",
    "    \n",
    "    def is_legal(self):\n",
    "        if any(long_card.name == \"The Moon\" for long_card in self.gs.opp.battlefield):  # Checking for The Moon (id 3)\n",
    "            return False, ERROR_ENEMY_HAS_THE_MOON  #  If opponent has the moon, can't play power cards\n",
    "        elif self.gs.me.power_plays_left < 1:\n",
    "            return False, ERROR_CANT_PLAY_ANOTHER_POWER_CARD\n",
    "        if self.gs.me.player_type.startswith(\"computer_ai\"):\n",
    "            if self.gs.me.name == \"monster\" and (self.resolving_card.name == \"Go All In\" or self.resolving_card.name == \"The 'Ol Switcheroo\"):  # The AI should never, ever do this\n",
    "                return False, ERROR_ACTION_WITHELD_FROM_AI\n",
    "            if self.gs.me.name == \"hero\" and self.resolving_card.name == \"Awakening\" and sum(1 for card in self.gs.me.hand if card.name == \"Awakening\") == 1:\n",
    "                return False, ERROR_ACTION_WITHELD_FROM_AI  # The AI should never play Awakening face down if they don't have another in their hand they can play\n",
    "        return True, None\n",
    "    \n",
    "    def execute(self):\n",
    "        #print(\"playing card face down: \" + self.card.name)\n",
    "        self.gs.me.play_power_card(self.gs, self.resolving_card)\n",
    "        self.reset()\n",
    "\n",
    "    def get_tempo(self):\n",
    "        if self.gs.me.name == \"hero\":\n",
    "            if self.resolving_card.card_type == \"long\":  # Long cards, especially Dragon and Pixie, are usually good to play face down, since they can combo with Awakening.\n",
    "                if self.resolving_card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\"]:\n",
    "                    self.gs.tempo += 2  # These are the best cards to play face down, since they can be flipped with Awakening\n",
    "                else:  # When sacrificing something else condition\n",
    "                    if any(card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\"] for card in self.gs.me.hand):\n",
    "                        self.gs.tempo -= 1  # You should always prioritize playing the heavy hitters face down\n",
    "                    else:\n",
    "                        self.gs.tempo += 1  # But if you don't have any heavy hitters, it's good to play these face down\n",
    "            else:  # Short cards condition\n",
    "                if self.resolving_card.name in [\"Awakening\", \"Healthy Eating\", \"Reconsider\"]:\n",
    "                    self.gs.tempo -= 1   # These are short cards you want to play yourself and not save for later, especially not Awakening. Awakening is made impossible to play face-down above in is_legal if there's only 1 in hand.\n",
    "                else:\n",
    "                    self.gs.tempo += 0  # Other short cards are fine to save for later, so you can play them after you play Awakening\n",
    "                if any(card for card in self.gs.me.hand if card.card_type == \"long\"):\n",
    "                    self.gs.tempo -= 1  # You shouldn't play short cards face down if you could play a long card face down\n",
    "        elif self.gs.me.name == \"monster\":\n",
    "            if self.resolving_card.name in [\"Fold\", \"Poker Face\"]:\n",
    "                self.gs.tempo -= 1  # These are not good cards to play face down; please note: Go All In and The 'Ol Switcheroo are actually not possible to play face down because of is_legal() above.\n",
    "            elif self.resolving_card.name == \"Cheap Shot\":\n",
    "                self.gs.tempo += 0  # Not good or bad to play face down\n",
    "            else:\n",
    "                self.gs.tempo += 1  # It's good to play other cards like Peek, Ultimatum, and Monster's Pawn face down, since they are weak and you only need one Monster's Pawn\n",
    "        if self.gs.me.power_plays_made_this_turn >= 1:  # If you have already played a power card...\n",
    "            self.gs.tempo += 1  # It is good to play multiple power cards in a turn with Healthy Eating, which allows this\n",
    "        if len(self.gs.me.power_cards) <= 2:\n",
    "            self.gs.tempo += 1  # Playing any power card yields tempo, but you only need around 3 (4-5 max, if you are playing Draong and Pixie face-up, which is usually not good)\n",
    "        elif len(self.gs.me.power_cards) >= 4:\n",
    "            self.gs.tempo -= 1  # You don't need more than 5 power cards\n",
    "        # These are for trying to teach the AI to clear out long cards:\n",
    "        if self.resolving_card.name == \"Poker Face\" and any(card.name in [\"A Playful Pixie\", \"A Pearlescent Dragon\", \"Monster's Pawn\"] for card in self.gs.opp.battlefield) and self.gs.me.power >= 2:\n",
    "            self.gs.tempo -= 2  # You should play Poker Face *face-up*, not face-down, if it is in your hand, you have the power, and your opponent has a threatening long card\n",
    "        elif self.resolving_card.name == \"Cheap Shot\" and any(card.name in [\"The Sun\", \"The Moon\"] for card in self.gs.opp.battlefield) and self.gs.me.power >= 2:\n",
    "            self.gs.tempo -= 2  # You should play Cheap Shot *face-up*, not face-down, if it is in your hand, you have the power, and your opponent has a great target for it\n",
    "        return self.gs.tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b43ea8",
   "metadata": {},
   "source": [
    "Although this next code cell doesn't seem straightforward, it actually makes the game flow really smoothly and intuitively when actually played.\n",
    "\n",
    "Here is how it works (this is complicated, so it's not super important to read this section). The code cell below contains helpers to make the cards, decks, and actions. Cards and decks are made with the create_card() and build_deck() functions. Making actions is harder. As previously noted, the game phases coordinate multi-step cards that require several choices to resolve, like playing the card Last Stand face-up, where the player needs to choose from three different cards in their graveyard to shuffle into their deck - each of those choices corresponds to a game phase, specifically, PHASE_SELECTING_GRAVEYARD_CARD. The action_id is a number from 0-40, which has a different meaning depending on the situation. For example, action_id 40 ends the turn if the game phase is PHASE_PENDING_INPUT, but does nothing if the game phase is PHASE_SELECTING_GRAVEYARD_CARD. In most game phases, action_ids 0-39 mean selecting one of the 40 unique cards from a card list, like the graveyard in the case of Last Stand. If the card is not in that list, then the action is illegal. Different combinations of game_phase and action_id create a different action when the create_action() function is called. The ACTION_MAP is used to figure what action *subclass* to create in a given situation, based on the game rules and flow.\n",
    "\n",
    "All of this was done so that the AI only has to make one choice at a time. This makes training much more simple, since all it has to do is choose a number from 0-40. (The output vector of the neural network corresponds to the action_id.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eae5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_card(name, card_id, uid, owner, card_type, power_cost, health, card_text):\n",
    "    card_name_to_effect = {\n",
    "        \"Awakening\": Awakening,\n",
    "        \"Healthy Eating\": HealthyEating,\n",
    "        \"A Playful Pixie\": APLayfulPixie,\n",
    "        \"A Pearlescent Dragon\": APearlescentDragon,\n",
    "        \"Last Stand\": LastStand,\n",
    "        \"Reconsider\": Reconsider,\n",
    "        \"Noble Sacrifice\": NobleSacrifice,\n",
    "        \"Monster's Pawn\": MonstersPawn,\n",
    "        \"Power Trip\": PowerTrip,\n",
    "        \"Poker Face\": PokerFace,\n",
    "        \"Cheap Shot\": CheapShot,\n",
    "        \"The 'Ol Switcheroo\": TheOlSwitcheroo,\n",
    "        \"Ultimatum\": Ultimatum,\n",
    "        \"Peek\": Peek\n",
    "    }  # Not all cards need their own subclass\n",
    "\n",
    "    CardClass = card_name_to_effect.get(name, Card)\n",
    "    return CardClass(name, card_id, uid, owner, card_type, power_cost, health, card_text)\n",
    "\n",
    "def build_decks():\n",
    "    hero_deck = []\n",
    "    monster_deck = []\n",
    "    uid = 0\n",
    "    card_id = 0\n",
    "    for quantity, name, owner, power_cost, card_type, health, card_text in card_data:\n",
    "        for i in range(quantity):\n",
    "            if owner == \"hero\":\n",
    "                    card = create_card(name, card_id, uid, owner, card_type, power_cost, health, card_text)\n",
    "                    uid += 1  # Increment uid for each unique card\n",
    "                    hero_deck.append(card)\n",
    "            elif owner == \"monster\":\n",
    "                    card = create_card(name, card_id, uid, owner, card_type, power_cost, health, card_text)\n",
    "                    uid += 1\n",
    "                    monster_deck.append(card)\n",
    "        card_id += 1  # Increment card_id for each new card name\n",
    "    return hero_deck, monster_deck\n",
    "\n",
    "num_game_phases = len(game_phases)\n",
    "# Create Matrix of size [i][j], where i = num_actions and j = num_game_phases\n",
    "ACTION_MAP = [[None for j in range(num_game_phases)] for i in range(num_actions)]\n",
    "\n",
    "def action_map_helper(game_phase, SelectFromClass=None, choosing_up_down=False, \n",
    "                      can_target_players=False, can_end_turn=False, can_get_card_info=False, can_cancel=False):\n",
    "    \"\"\"Based on a set of parameters, fills in the ACTION_MAP for a specific game phase.\"\"\"\n",
    "    j = game_phases.index(game_phase)  # phase_id\n",
    "    if choosing_up_down:\n",
    "        ACTION_MAP[0][j] = PlayFaceUp\n",
    "        ACTION_MAP[1][j] = PlayFaceDown\n",
    "    if SelectFromClass:\n",
    "        for i in range(num_actions):\n",
    "            ACTION_MAP[i][j] = SelectFromClass\n",
    "    if can_target_players:  # Might need to give these their own slots in order to not confuse AI\n",
    "        ACTION_MAP[-4][j] = TargetHero\n",
    "        ACTION_MAP[-3][j] = TargetMonster\n",
    "    if can_end_turn:\n",
    "        ACTION_MAP[-2][j] = EndTurn\n",
    "    if can_get_card_info:\n",
    "         ACTION_MAP[-2][j] = GetCardInfo  # Same spot as EndTurn, but doesn't matter\n",
    "    if can_cancel:\n",
    "        ACTION_MAP[-1][j] = Cancel\n",
    "\n",
    "# Now fill in the action map for each game phase using the action_map_helper.\n",
    "action_map_helper(PHASE_AWAITING_INPUT, SelectFromHand, can_end_turn=True)\n",
    "action_map_helper(PHASE_VIEWING_CARD_INFO, can_cancel=True)\n",
    "action_map_helper(PHASE_PLAYING_SELECTED_CARD, choosing_up_down=True, can_get_card_info=True, can_cancel=True)  # Can only get card info from this menu location\n",
    "action_map_helper(PHASE_REORDERING_DECK_TOP3, SelectFromDeckTop3)  # Can't cancel due to revealed info\n",
    "action_map_helper(PHASE_SACRIFICING_LONG_CARD, SelectFromOwnBattlefield, can_cancel=True)\n",
    "action_map_helper(PHASE_DISCARDING_CARD_FROM_OPP_HAND, SelectFromOppHand)  # Can't cancel since important info is revealed\n",
    "action_map_helper(PHASE_SELECTING_GRAVEYARD_CARD, SelectFromGraveyard, can_cancel=True)\n",
    "action_map_helper(PHASE_CHOOSING_GO_ALL_IN_TARGET, can_target_players=True, can_cancel=True)\n",
    "action_map_helper(PHASE_CHOOSING_FOLD_TARGET, can_target_players=True, can_cancel=True)\n",
    "action_map_helper(PHASE_CHOOSING_POKER_FACE_TARGET, SelectFromBattlefield, can_target_players=True, can_cancel=True)\n",
    "action_map_helper(PHASE_CHOOSING_CHEAP_SHOT_TARGET, SelectFromBattlefield, can_target_players=True, can_cancel=True)\n",
    "action_map_helper(PHASE_CHOOSING_ULTIMATUM_CARD, SelectFromDeck)  # If cancel was here, you would need to shuffle after looking at your deck. That's an extra step to the reset() function\n",
    "action_map_helper(PHASE_OPP_CHOOSING_FROM_ULTIMATUM, SelectFromUltimatum)  # Can't cancel due to lack of choice for opponent\n",
    "action_map_helper(PHASE_CHOOSING_FROM_DECK_TOP2, SelectFromDeckTop2)  # Can't cancel due to revealed information\n",
    "action_map_helper(PHASE_HAND_FULL_DISCARDING_CARD, SelectFromHand, can_cancel=True)\n",
    "# Fill in the rest of the game phases with appropriate actions\n",
    "\n",
    "def create_action(gs, action_id):\n",
    "    phase_id = game_phases.index(gs.game_phase)\n",
    "    action_class = ACTION_MAP[action_id][phase_id]\n",
    "\n",
    "    if action_class is None:\n",
    "        return InvalidAction(gs, action_id)  \n",
    "    else:\n",
    "        action = action_class(gs, action_id)\n",
    "    return action\n",
    "\n",
    "# Test:\n",
    "# j = 1\n",
    "# for i in range(num_actions):\n",
    "#     print(f\"Action {i}: {ACTION_MAP[i][j] if ACTION_MAP[i][j] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a624455",
   "metadata": {},
   "source": [
    "This code cells displays the information needed and actions available for a person to play the game in the console. It was mostly used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28fc132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def display_info(gs):\n",
    "    # Red and purple for Monser, green and yellow for hero\n",
    "    if gs.turn_priority == \"monster\":\n",
    "        print(f\"\\033[91m\\n=== {gs.turn_priority.upper()}'s TURN (Turn {gs.turn_number}) ===\\033[94m\")\n",
    "    else:\n",
    "        print(f\"\\033[92m\\n=== {gs.turn_priority.upper()}'s TURN (Turn {gs.turn_number}) ===\\033[93m\")\n",
    "    print(f\"Turn Number: {gs.turn_number}\")\n",
    "    print(f\"Game Phase: {gs.game_phase}\")\n",
    "    print(f\"My Health: {gs.me.health} | My Deck Size: {len(gs.me.deck)} | My Power: {gs.me.power}\")\n",
    "    print(f\"Opp Health: {gs.opp.health} | Opp Deck Size: {len(gs.opp.deck)} | Opp Hand Size: {len(gs.opp.hand)} | Opp Power Cards: {len(gs.opp.power_cards)}\")\n",
    "\n",
    "    # These are for extra info not always present\n",
    "    # Noble Sacrifice hand reveal\n",
    "    if gs.game_phase == PHASE_DISCARDING_CARD_FROM_OPP_HAND:  \n",
    "        print(\"Opp hand: \", [card.name for card in gs.opp.hand])\n",
    "    # Peek top2 reveal\n",
    "    if gs.game_phase == \"choosing from Peek\":\n",
    "        top2 = gs.me.deck[:2]\n",
    "        print(\"My deck top 2 cards: \", [card.name for card in top2])\n",
    "    # Ultimatum deck reveal\n",
    "    if gs.game_phase == PHASE_CHOOSING_ULTIMATUM_CARD:  \n",
    "        print(\"My deck: \", [card.name for card in gs.me.deck])\n",
    "    # Ultimatum ultimatum\n",
    "    if gs.game_phase == PHASE_OPP_CHOOSING_FROM_ULTIMATUM:  \n",
    "        print(\"Opp Ultimatum: \", [card.name for card in gs.cache[1:3]])\n",
    "    # Reconsider reveal\n",
    "    if gs.game_phase == PHASE_REORDERING_DECK_TOP3:\n",
    "        top2 = gs.me.deck[:3]\n",
    "        print(\"My deck top 3 cards: \", [card.name for card in top2])\n",
    "\n",
    "    # Standard info\n",
    "    if gs.me.hand:\n",
    "        print(\"My Hand:\", [card.name for card in gs.me.hand])\n",
    "    if gs.me.power_cards:\n",
    "        print(\"My Power Cards:\", [card.name for card in gs.me.power_cards])\n",
    "    if gs.me.battlefield:\n",
    "        print(\"My Battlefield:\", [(card.name, card.health) for card in gs.me.battlefield])\n",
    "    if gs.opp.battlefield:\n",
    "        print(\"Opp Battlefield:\", [(card.name, card.health) for card in gs.opp.battlefield])\n",
    "    if gs.me.graveyard:\n",
    "        print(\"My Graveyard:\", [card.name for card in gs.me.graveyard])\n",
    "    if gs.opp.graveyard:\n",
    "        print(\"Opp Graveyard:\", [card.name for card in gs.opp.graveyard])\n",
    "    if gs.me.monsters_pawn_buff:\n",
    "        print(\"Monster's Pawn buff is active\")\n",
    "    if gs.cache:\n",
    "        print(f\"\\033[96mCache: {[card.name for card in gs.cache]}\")\n",
    "    # if gs.turn_history:\n",
    "    #     move_tuple = gs.turn_history[0]\n",
    "    #     phase_id = move_tuple[0]\n",
    "    #     action_id = move_tuple[1]\n",
    "    #     print(f\"\\033[96mPrevious Move: {phase_id} {action_id}\")\n",
    "    # Reset the colors\n",
    "    print(\"\\033[0m\")  \n",
    "\n",
    "    # Print card info in a basic way. Could be amended to display card art as well.\n",
    "    if gs.game_phase == PHASE_VIEWING_CARD_INFO:\n",
    "        print(f\"\\033[95mPower Cost: {gs.cache[0].power_cost}\\n{gs.cache[0].card_text}\\033[0m\\n\")  # cache[0] is the resolving card that we need to access text from\n",
    "\n",
    "def display_actions(gs):\n",
    "    print(\"Available Actions:\")\n",
    "    \n",
    "    for action_id in range(num_actions):  # Assuming 20 possible actions\n",
    "        # print(\"Creating action: \", action_id)\n",
    "        action = create_action(gs, action_id)\n",
    "        legal, error = action.is_legal()\n",
    "        \n",
    "        if legal:\n",
    "            extra_info = \"\"\n",
    "            action_name = type(action).__name__  # Get the class name\n",
    "            if action_name in \"SelectFromHand\":\n",
    "                extra_info = f\": {action.resolving_card.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromBattlefield\":\n",
    "                extra_info = f\": {action.target.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromOwnBattlefield\":\n",
    "                extra_info = f\": {action.sacrifice.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromOppHand\":\n",
    "                extra_info = f\": {action.discard.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromDeckTop2\":\n",
    "                extra_info = f\": {action.selected_card.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromGraveyard\":\n",
    "                extra_info = f\": {action.selected_card.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromDeck\":\n",
    "                extra_info = f\": {action.selected_card.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromUltimatum\":\n",
    "                extra_info = f\": {action.selected_card.name}\" if action.card_list else \"\"\n",
    "            if action_name == \"SelectFromDeckTop3\":\n",
    "                extra_info = f\": {action.selected_card.name}\" if action.card_list else \"\"\n",
    "            print(f\"[{action_id}] {action_name} {extra_info}\") \n",
    "        elif error != ERROR_INVALID_SELECTION:  # QOL, error invalid shows up too often and don't need to see it\n",
    "            print(f\"[{action_id}] (Invalid) - {error}\")\n",
    "        \n",
    "    print(\"\")  # For the newline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7647fb",
   "metadata": {},
   "source": [
    "The purpose of the cell below is to create a rich feature vector for the reinforcement AI to learn on. Let me explain. The reinforcement AI is an artificial neural network, and artificial neural networks can only understand numbers. So how can someone turn the entire game into numbers? Well, one way to do this is to take the GameState and use various \"encoders\" that take the information held there and spit out some numbers that represent them. Some things are easy, like health, power, and deck size, which are already numbers. But how does something like a card get turned into a number? \n",
    "\n",
    "Well, this is where one-hot encoding comes in. This code cell has various 1-hot encoders that turn the player's zones into a vector that has one spot for every card that turns to 1 or 0 based on whether that card is available in that zone. \n",
    "\n",
    "Other information included in this feature vector is the game_phase, the cache, information that is revealed by certain cards like Peek and Reconsider, and other information I thought would be useful for the AI to know, like the degree of uncertainty of the current GameState.\n",
    "\n",
    "Perhaps the most important information, the potential tempo for every available action, is not included in this function. For reasons of speed, it is faster for the computer to include that during the AI computations, found in the next section. \n",
    "\n",
    "Speaking of speed, it's very important for all of this code to be as quick and simple to compute as possible, because training an AI often takes thousands of simulated games to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da26329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file needs to return a vector of constant size encoding almost all gamestate information\n",
    "# Input Vector Encoder\n",
    "\n",
    "def measure_gs():\n",
    "    # Build some temp variables to build a dummy gs, get vector, then get vector length\n",
    "    temp_hero_deck, temp_monster_deck = build_decks()\n",
    "    temp_hero = Player(\"hero\", temp_hero_deck)\n",
    "    temp_monster = Player(\"monster\", temp_monster_deck)\n",
    "    temp_gs = GameState(temp_hero, temp_monster)\n",
    "    return len(gs_to_vector(temp_gs))\n",
    "\n",
    "def encode_1hot(card_list):\n",
    "    # Can be configured based on 18 card_ids or 40 unique cards, just depending on what the AI can learn better.\n",
    "    # 18- and 40-card representations each have their pros and cons. The 18-card representation is about twice as small, but loses the specificity of the 40-card representation.\n",
    "    # Also, the final action vector (num_actions) uses the 40-card representation, which does not match the 18-card input vector representation. This may or may not have an impact. Probably not, but worth including.\n",
    "    one_hot_vector = [0] * len(card_data)  # 18 cards with different names/card_ids\n",
    "    # one_hot_vector = [0] * num_cards  # or 40 unique cards\n",
    "    for card in card_list:\n",
    "        index = card.card_id  # 18 cards by card_id\n",
    "        # index = card.uid  # 40 cards by uid\n",
    "        one_hot_vector[index] += 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def encode_battlefield_1hot(battlefield):\n",
    "    # This returns a one-hot vector with every long card in a battlefield, followed by their health. \n",
    "    # For AIs, the order of information doesn't matter so long as the information is there.\n",
    "    # long_cards_vector is a list of the uid of every unique long card\n",
    "    one_hot_vector = [0] * len(long_cards_vector) * 2  # times two to make room for health values of long cards\n",
    "    for long_card in battlefield:\n",
    "        if long_card.uid in long_cards_vector:\n",
    "            index = long_cards_vector.index(long_card.uid)\n",
    "            one_hot_vector[index] += 1\n",
    "            one_hot_vector[index+len(long_cards_vector)] = long_card.health  # Might want to normalize this value\n",
    "    return one_hot_vector\n",
    "\n",
    "def encode_game_phase_1hot(phase_id=None):\n",
    "    one_hot_vector = [0] * len(game_phases)\n",
    "    if phase_id:\n",
    "        one_hot_vector[phase_id] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def encode_action_1hot(action_id=None):\n",
    "    one_hot_vector = [0] * (num_actions - 1)  # -1 since computers can't cancel and it would be wasted space\n",
    "    if action_id:\n",
    "        one_hot_vector[action_id] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def gs_to_vector(gs, show_reveals=True, show_phase=True, show_cache=True):\n",
    "    x = []  # Populate this vector with everything a player can see--in one-hot format so computers/ais can read it easily\n",
    "    \n",
    "    # There are 5 zones per player: hand, deck, power cards, graveyard, and battlefield\n",
    "    # Friendly zones:\n",
    "    x += [len(gs.me.hand)/6]\n",
    "    x += encode_1hot(gs.me.hand)\n",
    "    x += [len(gs.me.deck)/16]\n",
    "    x += encode_1hot(gs.me.deck)  # You know what's in your deck, but not the order, and this doesn't encode the order\n",
    "    x += [len(gs.me.power_cards)/6]\n",
    "    x += encode_1hot(gs.me.power_cards)\n",
    "    x += [len(gs.me.graveyard)/20]\n",
    "    x += encode_1hot(gs.me.graveyard)\n",
    "    x += [len(gs.me.battlefield)/8]\n",
    "    x += encode_battlefield_1hot(gs.me.battlefield)  # This only includes long cards, so might want to shorten it to just the long cards (num_long_cards)\n",
    "\n",
    "    # Enemy zones:\n",
    "    x += [len(gs.opp.hand)/6]  # Can't see enemy hand\n",
    "    x += [len(gs.opp.deck)/16]  # Can't see inside\n",
    "    x += [len(gs.opp.power_cards)/6]  # Power cards are hidden\n",
    "    x += encode_1hot(gs.opp.hand + gs.opp.deck + gs.opp.power_cards)  # These are all the enemy unseen cards, which, as a group, are known but not the order, and this doesn't encoder order\n",
    "    x += [len(gs.opp.graveyard)/20]\n",
    "    x += encode_1hot(gs.opp.graveyard)\n",
    "    x += [len(gs.opp.battlefield)/8]\n",
    "    x += encode_battlefield_1hot(gs.opp.battlefield)  # This includes health values\n",
    "\n",
    "    # Differences:\n",
    "    x += [len(gs.me.hand)/6 - len(gs.opp.hand)/6]\n",
    "    x += [len(gs.me.deck)/16 - len(gs.opp.deck)/16]\n",
    "    x += [len(gs.me.power_cards)/6 - len(gs.opp.power_cards)/6]\n",
    "    x += [len(gs.me.graveyard)/20 - len(gs.opp.graveyard)/20]\n",
    "    x += [len(gs.me.battlefield)/8 - len(gs.opp.battlefield)/8]\n",
    "    x += [gs.me.health/29 - gs.opp.health/29]\n",
    "\n",
    "    # Misc.:\n",
    "    x += [gs.me.health/29, gs.opp.health/29, gs.me.power/6, gs.me.power_plays_left, gs.uncertainty/32, gs.me.action_number/40,\n",
    "          int(gs.me.monsters_pawn_buff), int(gs.me.last_stand_buff), int(gs.opp.last_stand_buff), int(gs.me.going_first),\n",
    "          gs.turn_number/15, int(gs.card_played_this_turn), int(gs.short_card_played_this_turn), gs.short_term_reward(gs.me.name)]  # Some of these values have been somewhat arbitrarily scaled to around 1 - this should help the AI\n",
    "    \n",
    "    # Damage, deck burn, and lethal potential:\n",
    "    damage_potential = sum(4 for card in gs.me.hand if card.name == \"Poker Face\") + sum(2 for card in gs.me.hand if card.name == \"Cheap Shot\") + sum(5 for card in gs.me.hand if card.name == \"Go All In\")\n",
    "    x += [damage_potential/29]\n",
    "    x += [1] if gs.opp.health <= damage_potential else [0]\n",
    "    deck_burn_potential = sum(3 for card in gs.me.hand if card.name == \"Go All In\") + sum(2 for card in gs.me.hand if card.name == \"Fold\")\n",
    "    x += [deck_burn_potential/16]\n",
    "    x += [1] if len(gs.opp.deck) <= deck_burn_potential else [0]\n",
    "\n",
    "    # Heal/damage, deck burn potential (next turn):\n",
    "    damage_potential = sum(5 for card in gs.me.battlefield if card.name == \"A Pearlescent Dragon\")  # Heal/damage potential\n",
    "    x += [damage_potential/29]\n",
    "    x += [1] if gs.opp.health <= damage_potential else [0]\n",
    "    deck_burn_potential = sum(1 for card in gs.me.battlefield if card.name == \"A Playful Pixie\")  # Burn potential\n",
    "    x += [deck_burn_potential/16]\n",
    "    x += [1] if len(gs.opp.deck) <= deck_burn_potential else [0]\n",
    "\n",
    "    # Hand power cost average:\n",
    "    try:\n",
    "        x += [(sum(card.power_cost for card in gs.me.hand) / len(gs.me.hand))/6]\n",
    "    except:\n",
    "        x += [0]\n",
    "\n",
    "    # Power curve - six entries for being able to play cards at each power cost from 0 to 5:\n",
    "    max_cost = 5  # Dragon\n",
    "    for cost in range(max_cost + 1):\n",
    "        playable_move_at_cost = 0\n",
    "        if any(c.power_cost == cost for c in gs.me.hand):\n",
    "            if gs.me.power >= cost:\n",
    "                playable_move_at_cost = 1\n",
    "            elif gs.me.monsters_pawn_buff:\n",
    "                if any(c.power_cost == cost and c.card_type == \"short\" for c in gs.me.hand):\n",
    "                    playable_move_at_cost = 1\n",
    "        x.append(playable_move_at_cost)\n",
    "\n",
    "    # Card-specific reveals:\n",
    "    if show_reveals:  # This puts all of the situational reveal cards in the same few vector spots, which might not be ideal, but it saves resources.\n",
    "        # Reconsider\n",
    "        revealed = []\n",
    "        if gs.game_phase == PHASE_REORDERING_DECK_TOP3:\n",
    "            revealed = gs.me.deck[:3]\n",
    "        # Noble Sacrifice\n",
    "        elif gs.game_phase == PHASE_DISCARDING_CARD_FROM_OPP_HAND:\n",
    "            revealed = gs.opp.hand\n",
    "        # Ultimatum 1\n",
    "        elif gs.game_phase == PHASE_CHOOSING_ULTIMATUM_CARD:\n",
    "            revealed = gs.me.deck\n",
    "        # Ultimatum 2\n",
    "        elif gs.game_phase == PHASE_OPP_CHOOSING_FROM_ULTIMATUM:\n",
    "            revealed = gs.cache[1:3]\n",
    "        # Peek\n",
    "        elif gs.game_phase == PHASE_CHOOSING_FROM_DECK_TOP2:\n",
    "            revealed = gs.me.deck[:2]\n",
    "        x += encode_1hot(revealed)\n",
    "\n",
    "    # Next need to encode the current game phase:\n",
    "    if show_phase:\n",
    "        phase_id = game_phases.index(gs.game_phase)\n",
    "        x += [phase_id/15]\n",
    "        x += encode_game_phase_1hot(phase_id)\n",
    "\n",
    "    # It's also possible to add the cache to the mix:\n",
    "    if show_cache:\n",
    "        # Note: this does not encode the order of the cache, which can matter. \n",
    "        # Would need to add additional slots if order does matter significantly to the AI.\n",
    "        x += encode_1hot(gs.cache)  \n",
    "\n",
    "    # All of the above are mostly essential.\n",
    "\n",
    "    # Then return tha damn thing\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec67f44",
   "metadata": {},
   "source": [
    "## The Deep Reinforcement, \"Long Short-Term Memory\", Recurrent Neural Network Itself\n",
    "\n",
    "Perhaps the most important formula in this entire 3,000+ lines of code program is the REINFORCE formula: **Loss = -log(prob)\\*R**\n",
    "\n",
    "Loss = the measure of how well the AI is doing\n",
    "prob = the probability that the AI had of choosing the chosen action (this is a Monte-Carlo method)\n",
    "R = the reward\n",
    "\n",
    "The way this formula works is it increases the probability that the chosen action will be repeated in a similar situation **if it led to a positive reward** and it *decreases* the probability that the chosen action will be repeated in a similar situation **if it led to a negative reward**. In other words, actions that lead to positive rewards are repeated, while it stops doing actions that are punished. Sort of like training a dog. It will do what you reward, and stop doing what you punish. That puts it a bit harshly, but it gets the point across (the only difference is that all of this is using math).\n",
    "\n",
    "All of this is done using PyTorch, a very convenient, yet slightly maddening, Python library that lets you set up an artificial neural network. My neural network is a recurrent neural network. I made this decision because my game is extremely cyclical and recursive. It turned out to be a good choice. Specifically, I used a \"long short-term memory\" network that can remember turns that occurred far in the past.\n",
    "\n",
    "My network is a deep network because it has four layers: three recurrent layers and one feedforward layer. It works very well and pretty fast. I can train on 2,000 games in about ten minutes, which is about 80,000 learning updates per player. That's usually enough to teach the AI how to play the game to a good degree.\n",
    "\n",
    "I also used a lot of the typical tools people use to help their models: dropout, a learning rate scheduler, entropy, epochs, temperature annealing, L2 regularization, LayerNorm, and gradient clipping.\n",
    "\n",
    "There is a lot going on in this code cell. However, I hope the inline comments are enough to make clear what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3efae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import copy\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# To make the AI have the same starting weights and biases every time, do this:\n",
    "# torch.manual_seed(0)  \n",
    "# Set device to maximize GPU if possible. Doing .to(device) on a tensor sends that information to the GPU if \"cuda\" is available. However, that data transfer takes a little time.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # Recurrant neural policy network using REINFORCE loss\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__()\n",
    "        # Hyperparameters at the top:\n",
    "        self.input_size = measure_gs() + num_actions-1\n",
    "        self.lstm_size = kwargs[\"lstm_size\"]   # Number of hidden neurons in the recurrant neural network's internal layers.\n",
    "        self.num_lstm_layers = kwargs[\"num_lstm_layers\"]\n",
    "        self.feedforward_size = kwargs[\"feedforward_size\"]\n",
    "        self.dropout_rate = kwargs[\"dropout_rate\"]\n",
    "        self.long_term_gamma = kwargs[\"long_term_gamma\"]  # Discount factor for long-term reward that only show up at the end of the game, 0.95 seems good, affecting all actions.\n",
    "        self.short_term_gamma = kwargs[\"short_term_gamma\"]  # Discount factors for short-term rewards that show up during the game and need to dissipate backwards in time quickly.\n",
    "        self.epochs = kwargs[\"epochs\"]\n",
    "        self.temperature = kwargs[\"temperature\"]\n",
    "        self.entropy_coef = kwargs[\"entropy_coef\"]\n",
    "        self.negative_reward_clamp = kwargs[\"negative_reward_clamp\"]\n",
    "        # Then network layer architecture:\n",
    "        self.lstm = nn.LSTM(self.input_size, self.lstm_size, num_layers=self.num_lstm_layers, dropout=self.dropout_rate)  # This is the Long-Short-Term Memory recurrant neural network. Has complex internal workings.\n",
    "        self.ln_lstm = nn.LayerNorm(self.lstm_size)  # Normalize the output of the LSTM\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.lstm_size, self.feedforward_size)  # Normal fully-connected feedforward layer.\n",
    "        self.ln1 = nn.LayerNorm(self.feedforward_size)  # Normalize the layer's outputs/logits (very helpful since Poker Monster has a lot of randomness)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.feedforward_size, num_actions-1)  # Actor, or policy head\n",
    "        # The optimizer and LR scheduler:\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=kwargs[\"lr\"], weight_decay=kwargs[\"weight_decay\"])  # PyTorch's optimizer for the neural network. Adam or AdamW work well.\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=kwargs[\"T_0\"], T_mult=kwargs[\"T_mult\"], eta_min=kwargs[\"eta_min\"])\n",
    "        self.use_lr_scheduler = kwargs[\"use_lr_scheduler\"]\n",
    "        # Misc.:\n",
    "        self.name = name  # \"hero\" or \"monster\"\n",
    "        self.memory = None  # This will contain various variables and game data that are needed for learning.\n",
    "        self.reset_memory()  # (This clears and initializes the memory)\n",
    "        self.num_params = sum(p.numel() for p in self.parameters())  # Total number of parameters (network size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, prev_lstm_state):\n",
    "        # Forward pass of the neural network. This produces the main outputs for the network but this is not called directly from main(). Instead, sample_action is called.\n",
    "        lstm_output, new_lstm_state = self.lstm(x, prev_lstm_state)  # For LSTM, x shape must be: [seq_length, measure_gs()]. A bit tricky but addressed below before calling forward.\n",
    "        x1 = self.dropout1(self.ln_lstm(lstm_output[-1]))  # Apply LayerNorm to LSTM output\n",
    "        x2 = self.dropout2(torch.relu(self.ln1(self.fc1(x1))) + x1)  # Feedforward layer for ln1 and fc1, with residual\n",
    "        return self.fc2(x2), new_lstm_state\n",
    "\n",
    "    def reset_memory(self):\n",
    "        # Since LSTM requires an existing lstm_state, this initializes one with zeros that is used.\n",
    "        h0 = torch.zeros(self.num_lstm_layers, self.lstm_size).to(device)\n",
    "        c0 = torch.zeros(self.num_lstm_layers, self.lstm_size).to(device)\n",
    "        # Also (re)establish the memory with the lstm_state:\n",
    "        self.memory = {\n",
    "                    \"gs_vectors\": [],\n",
    "                    \"action_ids\": [],\n",
    "                    \"long_term_rewards\": [],\n",
    "                    \"short_term_rewards\": [],\n",
    "                    \"masks\": [],\n",
    "                    \"tempos\": [],\n",
    "                    \"entropies\": [],\n",
    "                    \"logprobs\": [],\n",
    "                    \"lstm_states\": [(h0, c0)],\n",
    "                }\n",
    "\n",
    "    def tempo_mask(self, gs):\n",
    "        # Removes illegal actions from the NN output logits, for before applying softmax, while also finding the tempos for each action (that will be added to the input vector)\n",
    "        mask = torch.tensor([0] * (num_actions-1), dtype=torch.float32).to(device)  # Must make everything used in PyTorch computations a tensor like this.\n",
    "        tempos = torch.tensor([0] * (num_actions-1), dtype=torch.float32).to(device)\n",
    "        # Check every action_id\n",
    "        for i in range(num_actions - 1):  # This loop is kind of slow but it's the only way.\n",
    "            # Make an action\n",
    "            action = create_action(gs, i)\n",
    "            # Check if action is legal\n",
    "            legal, reason = action.is_legal()\n",
    "            # If not legal, set index to -infinity; -infinity becomes probability 0 in softmax (won't be picked)\n",
    "            if not legal:\n",
    "                mask[i] = float('-inf')\n",
    "                tempos[i] = -2  # Large negative value\n",
    "            if legal:\n",
    "                tempos[i] = action.predict_tempo()/4  # Only simulate for legal tempos, else bug\n",
    "        # Return masked logits\n",
    "        return mask, tempos  # Save mask for training since making actions is intensive and saving gs is a bad idea\n",
    "\n",
    "    def sample_action(self, gs, training=False):\n",
    "        # This is the main function that is called from the main loop (outside here) to get an action based on NN inference.\n",
    "        # Input a gamestate (gs), returns an action_id (an integer).\n",
    "        ctx = torch.inference_mode() if self.epochs > 1 else nullcontext()\n",
    "        # This part turns off gradient calculations, since all of this will be recomputed in train(). Can also use 'with torch.no_grad():'. To speed things up. Also disables dropout.\n",
    "        with ctx:\n",
    "            # Get mask + simulated tempos (and save mask for later training)\n",
    "            mask, tempos = self.tempo_mask(gs)\n",
    "            # Get gs_vector\n",
    "            gs_vector = gs_to_vector(gs)\n",
    "            # Convert to tensor for PyTorch and move to GPU if available\n",
    "            x = torch.tensor(gs_vector, dtype=torch.float32).to(device)\n",
    "            # Concatenate x with possible tempos\n",
    "            x = torch.cat((x, tempos), dim=0)\n",
    "            # Reshape for lstm\n",
    "            x = x.unsqueeze(0)\n",
    "            # Retreive previous lstm_state\n",
    "            prev_lstm_state = self.memory[\"lstm_states\"][-1]\n",
    "            # Do forward pass\n",
    "            logits, new_lstm_state = self(x, prev_lstm_state)\n",
    "            # print(f\"Action {len(self.memory['action_ids'])-1} sample logits {logits}\")\n",
    "            # if self.name == \"hero\" and len(self.memory['action_ids']) == 0:\n",
    "            #     print(f\"SAMPLE Action {len(self.memory['action_ids'])} gs_vector = {gs_vector}\")\n",
    "            #     print(f\"SAMPLE Action {len(self.memory['action_ids'])} logits = {logits}\")\n",
    "            # Element-wise vector addition\n",
    "            masked_logits = logits + mask\n",
    "            # Apply softmax to get probabilities; lower temp is less random and chaotic and higher temp is more uniform. High temp is good for early game exploration, low temp is good for late game exploitation.\n",
    "            probs = F.softmax(masked_logits / (self.temperature + 1e-9), dim=0) \n",
    "            # Add a small epsilon to try to avoid floating-point/nan errors\n",
    "            probs = probs.clamp(min=1e-9)\n",
    "            # Create distribution from softmax probabilities\n",
    "            dist = torch.distributions.Categorical(probs=probs)  # Not a tensor\n",
    "            # Get entropy for later (increases exploration)\n",
    "            entropy = dist.entropy()\n",
    "            # Get a random sample (sample size=1); dist.sample returns the index of the sampled value. This is a Monte-Carlo approach to learning.\n",
    "            sample = dist.sample()  # Tensor\n",
    "            # Convert to integer\n",
    "            action_id = sample.item()\n",
    "            # Calculate logprob for training when epochs=1\n",
    "            logprob = torch.log(probs[action_id])\n",
    "            # Save important data to memory for training\n",
    "            if training:\n",
    "                self.memory[\"gs_vectors\"].append(torch.tensor(copy(gs_vector), dtype=torch.float32))  # Must copy\n",
    "                self.memory[\"action_ids\"].append(torch.tensor(action_id, dtype=torch.long))\n",
    "                self.memory[\"masks\"].append(mask)\n",
    "                self.memory[\"tempos\"].append(tempos)\n",
    "                self.memory[\"entropies\"].append(entropy)\n",
    "                self.memory[\"logprobs\"].append(logprob)\n",
    "                self.memory[\"lstm_states\"].append(new_lstm_state)\n",
    "\n",
    "            return action_id\n",
    "\n",
    "    def train_network(self, baseline, epochs=1):\n",
    "        # This is the training loop that is called after the game is over, data has been collected, and there is a winner (or tie)\n",
    "        # All of the actions that took place are recomputed N times, where N is the number of epochs, if N is greater than 1. If N=1, the data was computed during sample_action.\n",
    "        # This applies the learning algorithm N times to optimize for the algorithm called 'REINFORCE'\n",
    "        losses = []  # Will be returned to make a graph later\n",
    "\n",
    "        if not self.memory:\n",
    "            print(\"Memory is empty, not training\")\n",
    "            self.reset_memory()  # Critical to reset memory!\n",
    "            return [0] * self.epochs\n",
    "        # Save compute by not training if reward is null\n",
    "        if baseline == 0:\n",
    "            # print(\"Reward is 0, not training\")\n",
    "            self.reset_memory()\n",
    "            return [0] * self.epochs  # Empty losses\n",
    "\n",
    "        # print(f\"Training {self.name}\")\n",
    "\n",
    "        B = len(self.memory[\"action_ids\"])  # Number of actions\n",
    "\n",
    "        # These are vectorized representations for every step, retreived from memory.\n",
    "        # For example, gs_vectors[0] is the gs_vector for the 0th action.\n",
    "        # This vectorized approach is must faster for computers to compute, especially on GPUs.\n",
    "        gs_vectors = torch.stack(self.memory[\"gs_vectors\"]).to(device)  # [B, measure_gs()] = dims\n",
    "        action_ids = torch.tensor(self.memory[\"action_ids\"], dtype=torch.long).to(device)  # [B]\n",
    "        long_rewards = torch.stack(self.memory[\"long_term_rewards\"]).to(device)  # [B]\n",
    "        short_rewards = torch.stack(self.memory[\"short_term_rewards\"]).to(device)  # [B]\n",
    "        masks = torch.stack(self.memory[\"masks\"]).to(device)  # [B, num_actions - 1]\n",
    "        tempos = torch.stack(self.memory[\"tempos\"]).to(device)\n",
    "        entropies = torch.stack(self.memory[\"entropies\"]).to(device)\n",
    "        logprobs = torch.stack(self.memory[\"logprobs\"]).to(device)\n",
    "\n",
    "        # Calculating discounted reward signals (courtesy of Gemini).\n",
    "        reward_signals = torch.zeros(B).to(device)\n",
    "        # These will be propagated backwards in time to calculate the reward signal for every action.\n",
    "        long_discounted_sum = 0.0\n",
    "        short_discounted_sum = 0.0\n",
    "        # Multiply rewards element-wise by baseline\n",
    "        long_rewards = long_rewards*baseline\n",
    "        short_rewards = short_rewards*baseline\n",
    "        # Iterate backwards to calculate discounted returns efficiently\n",
    "        for t in reversed(range(B)):\n",
    "            long_discounted_sum = long_rewards[t] + self.long_term_gamma*long_discounted_sum\n",
    "            short_discounted_sum = short_rewards[t] + self.short_term_gamma*short_discounted_sum\n",
    "            # Their sum equals a reward signal\n",
    "            reward_signals[t] = long_discounted_sum + short_discounted_sum\n",
    "        # Clamp negative rewards to make them less punishing\n",
    "        reward_signals = reward_signals.clamp(min=self.negative_reward_clamp)\n",
    "        \n",
    "        # for reward in reward_signals:\n",
    "        #     print(f\"{self.name} Reward signal: {reward}\")\n",
    "        # print(f\"Cumulative Reward: {reward_signals.sum().item():.2f} {self.name}\")\n",
    "\n",
    "        # Vectorized learning:\n",
    "        if self.epochs > 1:\n",
    "            for epoch in range(self.epochs):\n",
    "                self.memory[\"lstm_states\"] = []\n",
    "                # Recomputing ALL lstm forward passes\n",
    "                # Remake h0 and c0\n",
    "                h0 = torch.zeros(self.num_lstm_layers, self.lstm_size).to(device)\n",
    "                c0 = torch.zeros(self.num_lstm_layers, self.lstm_size).to(device)\n",
    "                # Assemble initial hidden state\n",
    "                lstm_state = (h0, c0)\n",
    "                # Intialize logits vector\n",
    "                logits = []\n",
    "                # Forward pass for every action done again\n",
    "                for i in range(B):  # This for loop is very slow. Unfortunately, it cannot be vectorized because of the way recurrent neural networks are.\n",
    "                    # retreive correct step number\n",
    "                    gs_vector = gs_vectors[i]\n",
    "                    tempos_ = tempos[i]\n",
    "                    # Concatenate with tempos and shape for LSTM\n",
    "                    x = torch.cat((gs_vector, tempos_), dim=0).unsqueeze(0)\n",
    "                    # Forward using (in-place updating) lstm state and gs_vector for every step\n",
    "                    logits_, lstm_state = self(x, lstm_state)\n",
    "                    # Add to vector, shape [B, num_actions - 1]\n",
    "                    logits.append(logits_)\n",
    "                    # if self.name == \"hero\" and i == 0:\n",
    "                        # print(f\"TRAIN Action {i} gs_vector = {gs_vector}\")\n",
    "                        # print(f\"TRAIN Action {i} logits = {logits_}\")\n",
    "                    # print(f\"Action {i} Training logits: {logits}\")\n",
    "                # Stack logits\n",
    "                logits = torch.stack(logits)\n",
    "                # Mask logits using previous masks to save resources\n",
    "                masked_logits = logits + masks  # [B, num_actions - 1] For this to work, masks needs to have -inf at illegal indexes and 0 everywhere else\n",
    "                # Calculate probs like above in get_sample()\n",
    "                probs = F.softmax(masked_logits / (self.temperature + 1e-9), dim=1)  # [B, num_actions - 1]\n",
    "                # Add a small epsilon to try to avoid floating-point/nan errors\n",
    "                probs = probs.clamp(min=1e-9)\n",
    "                # Find probabilities of choosing the chosen actions using vectorized math and saved action_ids\n",
    "                chosen_action_probs = probs[torch.arange(B), action_ids]  # [B]\n",
    "                # Calculate policy loss using REINFORCE formula: -log(prob)*R\n",
    "                policy_loss = -torch.log(chosen_action_probs) * reward_signals.detach()  # [B]\n",
    "                # Add to total loss\n",
    "                total_loss = policy_loss.mean() - self.entropy_coef*entropies.mean()\n",
    "                # Append to logger\n",
    "                losses.append(total_loss.item())\n",
    "                # losses.append(self.optimizer.param_groups[0]['lr'])  # Optional to watch learning rates\n",
    "                # losses.append(rewards[-1])\n",
    "                # Begin PyTorch gradient descent learning algorithm\n",
    "                self.optimizer.zero_grad()\n",
    "                # Backpropagate using total_loss\n",
    "                total_loss.backward()\n",
    "                # Clip exploding gradients (especially important for LSTM or RNN networks)\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                # Apply changes\n",
    "                self.optimizer.step()\n",
    "\n",
    "        else:  # For epochs=1 case (much faster vectorization due to no for loops, and less overfitting)\n",
    "            policy_loss = -logprobs * reward_signals.detach()\n",
    "            total_loss = policy_loss.mean() - self.entropy_coef*entropies.mean()  # Use mean and not sum to avoid favoring short games\n",
    "            losses.append(total_loss.item())\n",
    "            # losses.append(self.optimizer.param_groups[0]['lr'])  # To watch learning rates\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        if self.use_lr_scheduler:\n",
    "            # Update only once per episode\n",
    "            self.scheduler.step()\n",
    "\n",
    "        # Finally, reset memory\n",
    "        self.reset_memory()\n",
    "        # print(losses)\n",
    "        return losses\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.state_dict(), f\"ai_weights_{name}.pth\")\n",
    "        print(f\"Saved PyTorch model to ai_{name}.pth\")\n",
    "\n",
    "    def load(self, name):\n",
    "        self.load_state_dict(torch.load(f\"ai_weights_{name}.pth\", weights_only=True))\n",
    "        print(f\"Loaded PyTorch model from ai_{name}.pth\")\n",
    "\n",
    "    def get_state_dict(self):\n",
    "        # For population-based training\n",
    "        return deepcopy(self.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea258a",
   "metadata": {},
   "source": [
    "## Where it all comes together\n",
    "\n",
    "The Main Class contains the central game loop in run_game(), as well as several training methods: \n",
    "1. A simple training loop\n",
    "2. A way to train the AIs only on wins (using only positive reinforcement turns out to be useful)\n",
    "3. A way to train on a population of past opponents (this was used by Google DeepMind to train AlphaZero and AlphaGo, and significantly reduces overfitting)\n",
    "And there's also a simple testing loop to test AI performance as a result of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from random import randint\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class Main:\n",
    "    def __init__(self, hyperparameters, **kwargs):\n",
    "        # Training parameters:\n",
    "        self.training_hero_type = kwargs[\"training_hero_type\"]\n",
    "        self.training_monster_type = kwargs[\"training_monster_type\"]\n",
    "        self.training_display = kwargs[\"training_display\"]\n",
    "        self.hero_training = kwargs[\"hero_training\"]\n",
    "        self.monster_training = kwargs[\"monster_training\"]\n",
    "        # Annealing:\n",
    "        self.anneal_temperature = kwargs[\"anneal_temperature\"]\n",
    "        self.anneal_entropy = kwargs[\"anneal_entropy\"]\n",
    "        # Testing parameters:\n",
    "        self.testing_best_of = kwargs[\"testing_best_of\"]\n",
    "        self.testing_hero_type = kwargs[\"testing_hero_type\"]\n",
    "        self.testing_monster_type = kwargs[\"testing_monster_type\"]\n",
    "        self.testing_display = kwargs[\"testing_display\"]\n",
    "        # Misc.\n",
    "        self.game_mode = kwargs[\"game_mode\"]\n",
    "        self.scale = kwargs[\"scale\"]\n",
    "        # Data from training:\n",
    "        self.hero_loss_data = []  # To be filled out after training and used for graphs\n",
    "        self.monster_loss_data = []\n",
    "        self.hero_game_data = []  # 1 for win, 0 for loss, 0.5 for tie. Inverted for Monster.\n",
    "        self.monster_game_data = []\n",
    "        # Create AIs\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.hero_ai = Network(name=\"hero\", **hyperparameters)\n",
    "        self.monster_ai = Network(name=\"monster\", **hyperparameters)\n",
    "        # Pool parameters:\n",
    "        self.pool_size = kwargs[\"pool_size\"]\n",
    "        self.save_frequency = kwargs[\"save_frequency\"]\n",
    "        self.hero_pool = deque(maxlen=self.pool_size)  # To store a history of hero weights\n",
    "        self.monster_pool = deque(maxlen=self.pool_size)  # To store a history of monster weights\n",
    "\n",
    "    def load_ai_weights(self):\n",
    "        # Load weights into AIs if network architecture matches\n",
    "        try:\n",
    "            self.hero_ai.load(\"hero\")\n",
    "        except:\n",
    "            print(\"No hero_ai to load\")\n",
    "        try:\n",
    "            self.monster_ai.load(\"monster\")\n",
    "        except:\n",
    "            print(\"No monster_ai to load\")\n",
    "        print(f\"Total parameters: {self.hero_ai.num_params}\")\n",
    "        print(f\"Input vector size: {self.hero_ai.input_size}\")\n",
    "\n",
    "    def run_game(self, player_type_hero, player_type_monster, display=True, train_hero=False, train_monster=False, train_only_on_wins=False):\n",
    "        # game_mode 0 is normal mode, game_mode 1 is \"Power Trip\" mode\n",
    "\n",
    "        hero_deck, monster_deck = build_decks()\n",
    "        hero = Player(\"hero\", hero_deck, player_type_hero)\n",
    "        monster = Player(\"monster\", monster_deck, player_type_monster)\n",
    "\n",
    "        # coin flip to see who goes first\n",
    "        coin_flip = randint(0, 1)\n",
    "        going_first = None\n",
    "        if coin_flip == 0:  # Heads is for Monster, obviously\n",
    "            going_first = \"monster\"\n",
    "            monster.going_first = True\n",
    "        else:\n",
    "            going_first = \"hero\"\n",
    "            hero.going_first = True\n",
    "\n",
    "        # Initialize the game state, resetting important variables like the game phase, cache, and turn history\n",
    "        gs = GameState(hero, monster, going_first, PHASE_AWAITING_INPUT, cache=[], game_mode=self.game_mode)\n",
    "        hero.shuffle()\n",
    "        monster.shuffle()\n",
    "        hero.draw(4)\n",
    "        monster.draw(4)\n",
    "\n",
    "        while gs.winner is None:      \n",
    "            # time.sleep(5)  # To slow it down\n",
    "            if display:\n",
    "                # NEED TO CONNECT THIS TO WEBSITE HERE (1/2)\n",
    "                display_info(gs)\n",
    "                display_actions(gs)\n",
    "\n",
    "            current_ai = self.hero_ai if gs.turn_priority == \"hero\" else self.monster_ai\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    # For people:\n",
    "                    if gs.me.player_type == \"person\":\n",
    "                        # NEED TO CONNECT THIS TO WEBSITE HERE (2/2)\n",
    "                        choice_number = int(input(\"Choose an action: \"))  # int from 0 to num_actions - 1\n",
    "                    # For a random opponent:\n",
    "                    elif gs.me.player_type == \"computer_random\":\n",
    "                        choice_number = randint(0, num_actions - 2)  # -2 because cancel is not available to computers\n",
    "                    # For a computer AI:\n",
    "                    elif gs.me.player_type == \"computer_ai\":\n",
    "                        if gs.me.name == \"hero\":\n",
    "                            choice_number = current_ai.sample_action(gs, train_hero)\n",
    "                        elif gs.me.name == \"monster\":\n",
    "                            choice_number = current_ai.sample_action(gs, train_monster)\n",
    "                    # Create the action\n",
    "                    action = create_action(gs, choice_number)\n",
    "                    # Test if legal\n",
    "                    legal, reason = action.is_legal()\n",
    "                    if legal:\n",
    "                        # If legal, execute action\n",
    "                        if display:\n",
    "                            print(f\"Executing action: {type(action).__name__} [{choice_number}]\")\n",
    "                        action.enact()\n",
    "                        # Append action results (reward)\n",
    "                        current_ai.memory[\"long_term_rewards\"].append(torch.tensor(gs.long_term_reward(current_ai.name), dtype=torch.float32))\n",
    "                        current_ai.memory[\"short_term_rewards\"].append(torch.tensor(gs.short_term_reward(current_ai.name), dtype=torch.float32))\n",
    "                        if display:\n",
    "                            print(f\"Action Tempo: {gs.tempo}\")\n",
    "                            if gs.killer_combo:\n",
    "                                print(f\"Nice Killer Combo!\")\n",
    "                        break\n",
    "                    else:\n",
    "                        # If not, give reason\n",
    "                        if gs.me.player_type == \"person\":\n",
    "                            print(f\"Action not legal: {reason}\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a number.\")\n",
    "        \n",
    "        # Print winner with ~color~\n",
    "        if display:\n",
    "            if gs.winner == \"monster\":\n",
    "                print(f\"\\033[91mGame Over! Winner: {gs.winner}\\033[0m\")\n",
    "            elif gs.winner == \"hero\":\n",
    "                print(f\"\\033[92mGame Over! Winner: {gs.winner}\\033[0m\")\n",
    "            else:  # tie\n",
    "                print(f\"Game Over! Winner: {gs.winner}\")\n",
    "        else:  # Still print a small letter\n",
    "            if gs.winner == \"monster\":\n",
    "                print(\"\\033[91mM\\033[0m\", end=\"\")\n",
    "            elif gs.winner == \"hero\":\n",
    "                print(\"\\033[92mH\\033[0m\", end=\"\")\n",
    "            else:  # tie\n",
    "                print(\"T\", end=\"\")\n",
    "\n",
    "        # After the game, option to train AI on results, and at the same time, get loss data:\n",
    "        if gs.hero.player_type == \"computer_ai\" and train_hero:\n",
    "            hero_baseline = 1\n",
    "            if gs.winner != \"hero\" and train_only_on_wins:\n",
    "                hero_baseline = 0\n",
    "            self.hero_loss_data.extend(self.hero_ai.train_network(baseline=hero_baseline))\n",
    "        else:\n",
    "            self.hero_loss_data.extend([0] * self.hyperparameters[\"epochs\"])  # So that the graphs don't break if player isn't computer_ai\n",
    "        \n",
    "        if gs.monster.player_type == \"computer_ai\" and train_monster:\n",
    "            monster_baseline = 1\n",
    "            if gs.winner != \"monster\" and train_only_on_wins:\n",
    "                monster_baseline = 0\n",
    "            self.monster_loss_data.extend(self.monster_ai.train_network(baseline=monster_baseline))\n",
    "        else:\n",
    "            self.monster_loss_data.extend([0] * self.hyperparameters[\"epochs\"])\n",
    "        \n",
    "        return gs.winner\n",
    "\n",
    "    def do_training_loop(self, best_of=1):\n",
    "        start_time = time.time()\n",
    "        hero_score = 0\n",
    "        monster_score = 0\n",
    "        for i in range(best_of):\n",
    "            winner = self.run_game(player_type_hero=self.training_hero_type, \n",
    "                                player_type_monster=self.training_monster_type, \n",
    "                                display=self.training_display, \n",
    "                                train_hero=self.hero_training, \n",
    "                                train_monster=self.monster_training)\n",
    "            if winner == \"hero\":\n",
    "                hero_score += 1\n",
    "                self.hero_game_data.append(1)\n",
    "                self.monster_game_data.append(0)\n",
    "            elif winner == \"monster\":\n",
    "                monster_score += 1\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(1)\n",
    "            elif winner == \"tie\":\n",
    "                # Ties count the same as a loss to fix a small bug: when using the baseline=1, the next player to win after a tie gets to train and the other doesn't\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(0)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\" [{i+1}/{best_of}]\")\n",
    "\n",
    "            if self.anneal_entropy:\n",
    "                starting_entropy = self.hyperparameters[\"entropy_coef\"]\n",
    "                fraction = 1 - (i+1)/best_of\n",
    "                current_entropy = fraction * starting_entropy\n",
    "                self.hero_ai.entropy_coef = current_entropy\n",
    "                self.monster_ai.entropy_coef = current_entropy\n",
    "\n",
    "            if self.anneal_temperature:\n",
    "                starting_temp = self.hyperparameters[\"temperature\"]\n",
    "                fraction = 1 - (i+1)/best_of\n",
    "                current_temp = fraction * starting_temp\n",
    "                self.hero_ai.temperature = current_temp\n",
    "                self.monster_ai.temperature = current_temp\n",
    "\n",
    "         # After training, reset temperature and entropy\n",
    "        self.hero_ai.entropy_coef = self.hyperparameters[\"entropy_coef\"]\n",
    "        self.monster_ai.entropy_coef = self.hyperparameters[\"entropy_coef\"]\n",
    "        self.hero_ai.temperature = self.hyperparameters[\"temperature\"]\n",
    "        self.monster_ai.temperature = self.hyperparameters[\"temperature\"]\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time-start_time\n",
    "        print(\"\\nFinished training games\")\n",
    "        print(f\"The Hero's score: {hero_score}\")\n",
    "        print(f\"The Monster's score: {monster_score}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return hero_score, monster_score\n",
    "\n",
    "    def train_on_population(self, best_of=1):\n",
    "        # This is the same kind of training loop what was used in AlphaZero and AlphaGo, where the AI trains against a pool of past opponents.\n",
    "        start_time = time.time()\n",
    "        hero_score = 0\n",
    "        monster_score = 0\n",
    "\n",
    "        for i in range(best_of):\n",
    "            opponent_pool = None\n",
    "\n",
    "            # Alternate training between Hero and Monster:\n",
    "            if i % 2:\n",
    "                # Hero is training, Monster is frozen opponent\n",
    "                learner_ai = self.hero_ai\n",
    "                opponent_ai = self.monster_ai\n",
    "                oppponent_pool = self.monster_pool\n",
    "                train_hero, train_monster = True, False\n",
    "            else:\n",
    "                # Monster is training, Hero is frozen opponent\n",
    "                learner_ai = self.monster_ai\n",
    "                opponent_ai = self.hero_ai\n",
    "                oppponent_pool = self.hero_pool\n",
    "                train_hero, train_monster = False, True\n",
    "\n",
    "            if opponent_pool:\n",
    "                random_weights = random.choice(oppponent_pool)\n",
    "                opponent_ai.load_state_dict(random_weights)\n",
    "\n",
    "            winner = self.run_game(player_type_hero=self.training_hero_type, \n",
    "                                player_type_monster=self.training_monster_type, \n",
    "                                display=self.training_display, \n",
    "                                train_hero=train_hero, \n",
    "                                train_monster=train_monster)\n",
    "\n",
    "            if (i+1) % self.save_frequency == 0:\n",
    "                # print(f\"Saving AI weights at game {i+1}\")\n",
    "                self.hero_pool.append(self.hero_ai.get_state_dict())\n",
    "                self.monster_pool.append(self.monster_ai.get_state_dict())\n",
    "\n",
    "            if winner == \"hero\":\n",
    "                hero_score += 1\n",
    "                self.hero_game_data.append(1)\n",
    "                self.monster_game_data.append(0)\n",
    "            elif winner == \"monster\":\n",
    "                monster_score += 1\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(1)\n",
    "            elif winner == \"tie\":\n",
    "                # Ties count the same as a loss to fix a small bug: when using the baseline=1, the next player to win after a tie gets to train and the other doesn't\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(0)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\" [{i+1}/{best_of}]\")\n",
    "\n",
    "            if self.anneal_entropy:\n",
    "                starting_entropy = self.hyperparameters[\"entropy_coef\"]\n",
    "                fraction = 1 - (i+1)/best_of\n",
    "                current_entropy = fraction * starting_entropy\n",
    "                self.hero_ai.entropy_coef = current_entropy\n",
    "                self.monster_ai.entropy_coef = current_entropy\n",
    "\n",
    "            if self.anneal_temperature:\n",
    "                starting_temp = self.hyperparameters[\"temperature\"]\n",
    "                fraction = 1 - (i+1)/best_of\n",
    "                current_temp = fraction * starting_temp\n",
    "                self.hero_ai.temperature = current_temp\n",
    "                self.monster_ai.temperature = current_temp\n",
    "\n",
    "        # After training, reset temperature and entropy\n",
    "        self.hero_ai.entropy_coef = self.hyperparameters[\"entropy_coef\"]\n",
    "        self.monster_ai.entropy_coef = self.hyperparameters[\"entropy_coef\"]\n",
    "        self.hero_ai.temperature = self.hyperparameters[\"temperature\"]\n",
    "        self.monster_ai.temperature = self.hyperparameters[\"temperature\"]\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time-start_time\n",
    "        print(\"\\nFinished training games\")\n",
    "        print(f\"The Hero's score: {hero_score}\")\n",
    "        print(f\"The Monster's score: {monster_score}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return hero_score, monster_score\n",
    "\n",
    "    def train_on_wins(self, name, cutoff):\n",
    "        # Train until the hero or monster reaches as many wins as the cutoff, and also train only on wins.\n",
    "        def anneal_entropy(score):\n",
    "            starting_entropy = self.hyperparameters[\"entropy_coef\"]\n",
    "            fraction = 1 - score/cutoff\n",
    "            current_entropy = fraction * starting_entropy\n",
    "            self.hero_ai.entropy_coef = current_entropy\n",
    "            self.monster_ai.entropy_coef = current_entropy\n",
    "\n",
    "        start_time = time.time()\n",
    "        hero_score = 0\n",
    "        monster_score = 0\n",
    "        i = 0\n",
    "        while (name == \"hero\" and hero_score < cutoff) or (name == \"monster\" and monster_score < cutoff):\n",
    "            winner = self.run_game(player_type_hero=self.training_hero_type, \n",
    "                                player_type_monster=self.training_monster_type, \n",
    "                                display=self.training_display, \n",
    "                                train_hero=self.hero_training, \n",
    "                                train_monster=self.monster_training,\n",
    "                                train_only_on_wins=True)\n",
    "            if winner == \"hero\":\n",
    "                hero_score += 1\n",
    "                self.hero_game_data.append(1)\n",
    "                self.monster_game_data.append(0)\n",
    "                if self.anneal_entropy and name == \"hero\":\n",
    "                    anneal_entropy(hero_score)\n",
    "            elif winner == \"monster\":\n",
    "                monster_score += 1\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(1)\n",
    "                if self.anneal_entropy and name == \"monster\":\n",
    "                    anneal_entropy(monster_score)\n",
    "            elif winner == \"tie\":\n",
    "                self.hero_game_data.append(0)\n",
    "                self.monster_game_data.append(0)\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                if name == \"hero\":\n",
    "                    print(f\" [\\033[92mH\\033[0m{hero_score}/{cutoff}]\")\n",
    "                elif name == \"monster\":\n",
    "                    print(f\" [\\033[91mM\\033[0m{monster_score}/{cutoff}]\")\n",
    "            i += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time-start_time\n",
    "        print(\"\\nFinished training games\")\n",
    "        print(f\"The Hero's score: {hero_score}\")\n",
    "        print(f\"The Monster's score: {monster_score}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return hero_score, monster_score\n",
    "\n",
    "    def do_testing_loop(self, best_of=1):\n",
    "        # After training, test against computer_random as benchmark with \"greedy\" temp settings\n",
    "        hero_score = 0\n",
    "        monster_score = 0\n",
    "        for _ in range(best_of):\n",
    "            winner = self.run_game(player_type_hero=self.testing_hero_type, \n",
    "                                player_type_monster=self.testing_monster_type, \n",
    "                                display=self.testing_display)\n",
    "            if winner == \"hero\":\n",
    "                hero_score += 1\n",
    "            if winner == \"monster\":\n",
    "                monster_score += 1\n",
    "\n",
    "        print(\"\\nFinished all testing games\")\n",
    "        print(f\"The Hero's score: {hero_score}\")\n",
    "        print(f\"The Monster's score: {monster_score}\\n\")\n",
    "\n",
    "        return hero_score, monster_score\n",
    "\n",
    "    def save_ai_weights(self):\n",
    "        # Saving the AI model\n",
    "        response = input(\"\\nSave AI models? [y/n]: \")\n",
    "        if response == \"y\":\n",
    "            self.hero_ai.save(\"hero\")\n",
    "            self.monster_ai.save(\"monster\")\n",
    "        else:\n",
    "            print(\"Models not saved\")\n",
    "\n",
    "    def plot_graphs(self):\n",
    "        hero_avg_wr = mean(self.hero_game_data)\n",
    "        monster_avg_wr = mean(self.monster_game_data)\n",
    "        print(f\"Hero average winrate: {hero_avg_wr}\")\n",
    "        print(f\"Monster average winrate: {monster_avg_wr}\")\n",
    "\n",
    "        # In case one set of data isn't present:\n",
    "        if not self.hero_loss_data:\n",
    "            self.hero_loss_data = [0] * len(self.monster_loss_data)\n",
    "        if not self.monster_loss_data:\n",
    "            self.monster_loss_data = [0] * len(self.hero_loss_data)\n",
    "\n",
    "        # Create Plotly figure\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 7), sharex=False)\n",
    "\n",
    "        # Loss over time graph\n",
    "        loss_graph_period = int(len(self.hero_loss_data)/self.scale)\n",
    "        data = {'hero_loss': self.hero_loss_data, 'monster_loss': self.monster_loss_data}\n",
    "        df = pd.DataFrame(data)\n",
    "        df['hero_loss_ma'] = df['hero_loss'].rolling(window=loss_graph_period).mean()\n",
    "        df['monster_loss_ma'] = df['monster_loss'].rolling(window=loss_graph_period).mean()\n",
    "        # Plot moving averages against index\n",
    "        ax1.plot(df.index, df['hero_loss_ma'], label='Hero Loss MA')\n",
    "        ax1.plot(df.index, df['monster_loss_ma'], label='Monster Loss MA')\n",
    "        ax1.set_title(f'Moving Average Loss (period={loss_graph_period})')\n",
    "        ax1.set_xlabel('Step')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        # ax1.set_yscale('symlog', linthresh=1e-1)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Win rate over time graph\n",
    "        wr_graph_period = int(len(self.hero_game_data)/self.scale) * self.hyperparameters[\"epochs\"]\n",
    "        hero_win_rate = pd.Series(self.hero_game_data).rolling(window=wr_graph_period).mean()\n",
    "        monster_win_rate = pd.Series(self.monster_game_data).rolling(window=wr_graph_period).mean()\n",
    "        # Plot Hero win rate moving average over time\n",
    "        ax2.plot(hero_win_rate, label=\"Hero Win Rate\")\n",
    "        ax2.plot(monster_win_rate, label=\"Monster Win Rate\")\n",
    "        ax2.set_title(\"Hero Win Rate Over Time\")\n",
    "        ax2.set_xlabel(\"Game #\")\n",
    "        ax2.set_ylabel(\"Win Rate\")\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "\n",
    "        # Adjust layout and show figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefcb9d0",
   "metadata": {},
   "source": [
    "This contains the hyperparameters I used to train my model, as well as the training schedule I set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1, FIGHT!\n",
      "\u001b[92mH\u001b[0m\u001b[92mH\u001b[0m\u001b[92mH\u001b[0m\u001b[92mH\u001b[0m\u001b[91mM\u001b[0m"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m main.training_hero_type = \u001b[33m\"\u001b[39m\u001b[33mcomputer_random\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m main.training_monster_type = \u001b[33m\"\u001b[39m\u001b[33mcomputer_ai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m main.train_on_wins(\u001b[33m\"\u001b[39m\u001b[33mmonster\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m250\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Pretrain Hero on random Monster\u001b[39;00m\n\u001b[32m     61\u001b[39m main.training_hero_type = \u001b[33m\"\u001b[39m\u001b[33mcomputer_ai\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 314\u001b[39m, in \u001b[36mMain.train_on_wins\u001b[39m\u001b[34m(self, name, cutoff)\u001b[39m\n\u001b[32m    312\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (name == \u001b[33m\"\u001b[39m\u001b[33mhero\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hero_score < cutoff) \u001b[38;5;129;01mor\u001b[39;00m (name == \u001b[33m\"\u001b[39m\u001b[33mmonster\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m monster_score < cutoff):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     winner = \u001b[38;5;28mself\u001b[39m.run_game(player_type_hero=\u001b[38;5;28mself\u001b[39m.training_hero_type, \n\u001b[32m    315\u001b[39m                         player_type_monster=\u001b[38;5;28mself\u001b[39m.training_monster_type, \n\u001b[32m    316\u001b[39m                         display=\u001b[38;5;28mself\u001b[39m.training_display, \n\u001b[32m    317\u001b[39m                         train_hero=\u001b[38;5;28mself\u001b[39m.hero_training, \n\u001b[32m    318\u001b[39m                         train_monster=\u001b[38;5;28mself\u001b[39m.monster_training,\n\u001b[32m    319\u001b[39m                         train_only_on_wins=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m winner == \u001b[33m\"\u001b[39m\u001b[33mhero\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m         hero_score += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mMain.run_game\u001b[39m\u001b[34m(self, player_type_hero, player_type_monster, display, train_hero, train_monster, train_only_on_wins)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gs.winner != \u001b[33m\"\u001b[39m\u001b[33mmonster\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m train_only_on_wins:\n\u001b[32m    155\u001b[39m         monster_baseline = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28mself\u001b[39m.monster_loss_data.extend(\u001b[38;5;28mself\u001b[39m.monster_ai.train_network(baseline=monster_baseline))\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m.monster_loss_data.extend([\u001b[32m0\u001b[39m] * \u001b[38;5;28mself\u001b[39m.hyperparameters[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mNetwork.train_network\u001b[39m\u001b[34m(self, baseline, epochs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# losses.append(self.optimizer.param_groups[0]['lr'])  # To watch learning rates\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m total_loss.backward()\n\u001b[32m    257\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    258\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henry\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henry\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m _engine_run_backward(\n\u001b[32m    354\u001b[39m     tensors,\n\u001b[32m    355\u001b[39m     grad_tensors_,\n\u001b[32m    356\u001b[39m     retain_graph,\n\u001b[32m    357\u001b[39m     create_graph,\n\u001b[32m    358\u001b[39m     inputs,\n\u001b[32m    359\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    360\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henry\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    825\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    826\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "!ls /  # Shows that you are connected to Paperspace cloud GPU\n",
    "# Sometimes bugs occur from using a remote kernel.\n",
    "\n",
    "hyperparameters = {\n",
    "    # Network architecture:\n",
    "    \"lstm_size\": 70,\n",
    "    \"num_lstm_layers\": 3,  # Ignore dropout warning if X=1\n",
    "    \"feedforward_size\": 70,\n",
    "    # Reward shaping:\n",
    "    \"long_term_gamma\": 0.95,  # Lower values decay the end-of-game reward to earlier turns faster\n",
    "    \"short_term_gamma\": 0.75,\n",
    "    \"negative_reward_clamp\": float('-inf'),  # Clamp negative rewards to this value to make them less punishing. All rewards below this value are set to this value (set to float('-inf') to disable)\n",
    "    # Learning rate parameters (LR scheduler):\n",
    "    \"lr\":1e-3,  # For 5 or more epochs, use 1e-4; for 1 epoch use 1e-3 (no scheduler)\n",
    "    \"use_lr_scheduler\": True,  # lr scheduler (cosine annealing)\n",
    "    \"T_0\": 250,  # lr cosine anneal period (repeats every X games with warm restarts)\n",
    "    \"T_mult\": 1,  # Multiply T_0 by this factor every time it restarts (default is 1)\n",
    "    \"eta_min\": 1e-5,  # Anneal from lr (above) to this lr\n",
    "    # Misc. Parameters:\n",
    "    \"dropout_rate\": 0.2,  # Randomly disables X% neurons during forward pass. Reduces overfitting, but too high a value adds a lot of noise to the loss.\n",
    "    \"weight_decay\": 0.01,  # This is L2 regularization, adds a term to the loss calculation that punishes large weights\n",
    "    \"epochs\": 1,  # 1 epoch is much faster than multiple because the torch gradient isn't recomputed\n",
    "    \"temperature\": 2,  # Adds a degree of randomness to sample_action. Lower values are deterministic, higher values are random.\n",
    "    \"entropy_coef\": 0.025,  # Higher values slow down learning, increase exploration, and slow convergence.\n",
    "}\n",
    "\n",
    "game_settings = {\n",
    "    # Training parameters:\n",
    "    \"training_hero_type\": \"computer_ai\",\n",
    "    \"training_monster_type\": \"computer_ai\",\n",
    "    \"training_display\": False,\n",
    "    \"hero_training\": True,\n",
    "    \"monster_training\": True,\n",
    "    # Annealing:\n",
    "    \"anneal_temperature\": False,\n",
    "    \"anneal_entropy\": False,  # Linearly reduce entropy from entropy_coef to 0 over the course of training\n",
    "    # Pooling parameters:\n",
    "    \"pool_size\": 25,  # How many AI weights to keep in the pool for training\n",
    "    \"save_frequency\": 25,\n",
    "    # Testing parameters:\n",
    "    \"testing_best_of\": 1,\n",
    "    \"testing_hero_type\": \"computer_ai\",\n",
    "    \"testing_monster_type\": \"computer_ai\",\n",
    "    \"testing_display\": True,\n",
    "    # Misc.:\n",
    "    \"game_mode\": 0,\n",
    "    \"scale\": 40  # Lower values are *less* zoomed in\n",
    "}\n",
    "\n",
    "main = Main(hyperparameters, **game_settings)\n",
    "# main.load_ai_weights()\n",
    "\n",
    "# Training program:\n",
    "for i in range(4):\n",
    "    print(f\"\\nRound {i+1}, FIGHT!\")\n",
    "    # Pretrain Monster on random Hero\n",
    "    main.training_hero_type = \"computer_random\"\n",
    "    main.training_monster_type = \"computer_ai\"\n",
    "    main.train_on_wins(\"monster\", 250)\n",
    "    # Pretrain Hero on random Monster\n",
    "    main.training_hero_type = \"computer_ai\"\n",
    "    main.training_monster_type = \"computer_random\"\n",
    "    main.train_on_wins(\"hero\", 250)\n",
    "    # Train Hero and Monster on a pool of their past opponents\n",
    "    main.training_hero_type = \"computer_ai\"\n",
    "    main.training_monster_type = \"computer_ai\"\n",
    "    # main.anneal_temperature = True\n",
    "    # main.anneal_entropy = True\n",
    "    main.train_on_population(500)\n",
    "    # Turn off annealing\n",
    "    # main.anneal_temperature = False\n",
    "    # main.anneal_entropy = False\n",
    "    main.hero_ai.temperature /= 2\n",
    "    main.monster_ai.temperature /= 2\n",
    "    main.hero_ai.entropy_coef /= 2\n",
    "    main.monster_ai.entropy_coef /= 2\n",
    "# After training, test the AI against each other\n",
    "main.hero_training = False\n",
    "main.monster_training = False\n",
    "main.hero_ai.temperature = 0.0001\n",
    "main.monster_ai.temperature = 0.0001\n",
    "main.do_testing_loop(1)\n",
    "main.plot_graphs()\n",
    "main.save_ai_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
